{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time_series",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VFAVEodC5OEeWmmwbIe7ZCtToBJT7Oji",
      "authorship_tag": "ABX9TyPXQMK4z7FAxAjwY7586QF+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravmarcin/GoogleColab/blob/main/Time_series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You have to type these two commands in your virtual env. ( without '!' marks ) to install the versions, which I used."
      ],
      "metadata": {
        "id": "atoqOcwTPt3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow==2.3.0\n",
        "! pip install keras==2.4.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EP3jdeasp3gI",
        "outputId": "efae7005-bc05-4280-8cab-80741cbfba0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.3.0\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 50 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.37.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.42.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "h5py",
                  "numpy",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.3) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "Successfully installed keras-2.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import libraries."
      ],
      "metadata": {
        "id": "aGHbqXr2QD8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Ignore the log about CPU usage\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# Ignore the log about repeated tf process\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8ArnlNacCet",
        "outputId": "9db0194d-50f0-44d7-b354-fc0d9e3d704e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a class with all functions"
      ],
      "metadata": {
        "id": "Rbnz3ltfQPAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesModel(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 col_oil=\"Oil Rate SC\",\n",
        "                 col_gas=\"Gas Mass Rate(CO2) SC\",\n",
        "                 col_water=\"Water Rate SC\",\n",
        "                 col_t=\"Time (day)\",\n",
        "                 dir_=\"/content/drive/MyDrive/Colab Notebooks/Existing_wells/\",\n",
        "                 dir_n_well=\"/content/drive/MyDrive/Colab Notebooks/New_well/\",\n",
        "                 col_por=\"Porosity \",\n",
        "                 col_per=\"Permeability\",\n",
        "                 col_fm=\"Fm Thickness\",\n",
        "                 col_bhp=\"BHP\",\n",
        "                 i_coor_col=\"i\",\n",
        "                 j_coor_col=\"j\",\n",
        "                 model_=LinearRegression(),\n",
        "                 n_old_obs=20,\n",
        "                 n_well=8,\n",
        "                 train_=0.4,\n",
        "                 valid_=0.2,\n",
        "                 test_=0.4,\n",
        "                 factor_=20,\n",
        "                 mod_update=200,\n",
        "                 w_model_init=0.5,\n",
        "                 w_model_update=0.5,\n",
        "                 lin_=True,\n",
        "                 lstm_=False,\n",
        "                 dense_=False,\n",
        "                 sec_order=False,\n",
        "                 pred_round=True,\n",
        "                 epochs=10,\n",
        "                 mod_temp_dir=\"./model_temp/\"\n",
        "                 ):\n",
        "        self.col_gas = col_gas\n",
        "        self.col_water = col_water\n",
        "        self.dir_ = dir_\n",
        "        self.dir_n_well = dir_n_well\n",
        "        self.col_time = col_t\n",
        "        self.i_coor_col = i_coor_col\n",
        "        self.j_coor_col = j_coor_col\n",
        "        self.col_coor = [i_coor_col, j_coor_col]\n",
        "        self.var_col = [col_oil, col_gas, col_water]\n",
        "        self.const_col = [col_por, col_per, col_fm, col_bhp]\n",
        "        self.col_ls = [col_oil, col_gas, col_water, col_por, col_per, col_fm, col_bhp]\n",
        "        self.model_ = model_\n",
        "        if n_old_obs > 3:\n",
        "            self.n_old_obs = n_old_obs\n",
        "        else:\n",
        "            self.n_old_obs = 3\n",
        "        self.n_well = n_well\n",
        "        self.train_ = train_\n",
        "        self.valid_ = valid_\n",
        "        self.test_ = test_\n",
        "        self.factor_ = factor_\n",
        "        self.mod_update = mod_update\n",
        "        self.w_model_init = w_model_init\n",
        "        self.w_model_update = w_model_update\n",
        "        self.sec_order = sec_order\n",
        "        self.pred_round = pred_round\n",
        "        self.epochs = epochs\n",
        "        self.mod_temp_dir = mod_temp_dir\n",
        "\n",
        "        self.lin_ = lin_\n",
        "        self.lstm_ = lstm_\n",
        "        self.dense_ = dense_\n",
        "\n",
        "        if self.lin_:\n",
        "            self.dense_ = False\n",
        "            self.lstm_ = False\n",
        "        elif self.dense_:\n",
        "            self.lstm_ = False\n",
        "        else:\n",
        "            self.lstm_ = True\n",
        "\n",
        "    \"\"\" Import data function \"\"\"\n",
        "\n",
        "    def import_data(self, dir_, roll=True, roll_par=60, ext_1=\"PRO-\", ext_2=\".csv\", pro_=\"01\"):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            dir_: a directory for data\n",
        "            rolling: a boolean value, if True then Gas, Oil, and Water values are rolled\n",
        "        Output:\n",
        "            pro_df_r: a true (real) pandas dataframe (df)\n",
        "            pro_df: a convert pandas dataframe (df)\n",
        "        \"\"\"\n",
        "\n",
        "        # read data from the dir_:\n",
        "        pro_df_r = pd.read_csv(dir_ + ext_1 + pro_ + ext_2, sep=',', encoding=\"ISO-8859-1\")\n",
        "        pro_df = pro_df_r.copy()\n",
        "\n",
        "        # roll the Gas, Oil and Water values:\n",
        "        if roll:\n",
        "            for col_ in self.var_col:\n",
        "                pro_df[col_] = \\\n",
        "                    pro_df[col_].rolling(roll_par, min_periods=1).mean()\n",
        "\n",
        "        return pro_df_r, pro_df\n",
        "\n",
        "    def normalization(self, pro_df_ls, mean_=None, sigma_=None, init_=True):\n",
        "\n",
        "        for j in range(len(self.const_col)):\n",
        "            if init_:\n",
        "                const_ = []\n",
        "                for i in range(self.n_well):\n",
        "                    const_.append(pro_df_ls[i][self.const_col[j]].to_numpy()[0])\n",
        "                const_ = np.array(const_)\n",
        "                sigma_ = np.std(const_)\n",
        "                mean_ = np.mean(const_)\n",
        "                for i in range(self.n_well):\n",
        "                    norm_col = (pro_df_ls[i][self.const_col[j]] - mean_) / sigma_\n",
        "                    pro_df_ls[i][self.const_col[j]] = norm_col\n",
        "            else:\n",
        "                norm_col = (pro_df_ls[self.const_col[j]] - mean_) / sigma_\n",
        "                pro_df_ls[self.const_col[j]] = norm_col\n",
        "\n",
        "        return pro_df_ls, [mean_, sigma_]\n",
        "\n",
        "    \"\"\" Get list of all dataframes \"\"\"\n",
        "\n",
        "    def get_df_list(self, dir_, norm_, init_=True, ext_1=\"PRO-\", roll=True, pro_=\"03\"):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            pro_n: a number of wells\n",
        "        Output:\n",
        "            pro_df_r_ls: a list of a true (real) pandas dataframe (n_df, df)\n",
        "            pro_df_ls: a list of a convert pandas dataframe (n_df, df)\n",
        "        \"\"\"\n",
        "\n",
        "        # create empty lists:\n",
        "        pro_df_r_ls, pro_df_ls = [], []\n",
        "\n",
        "        # loop trough the number of wells:\n",
        "        if init_:\n",
        "            for i in range(self.n_well):\n",
        "                pro_ = \"0\" + str(i + 1)\n",
        "\n",
        "                # import dataframes for a given well number:\n",
        "                pro_df_r, pro_df = self.import_data(dir_=dir_, roll=True, roll_par=60, ext_1=ext_1, ext_2=\".csv\",\n",
        "                                                    pro_=pro_)\n",
        "                pro_df_r_ls.append(pro_df_r)\n",
        "                pro_df_ls.append(pro_df)\n",
        "\n",
        "            pro_df_ls, norm_ = self.normalization(pro_df_ls=pro_df_ls, init_=init_)\n",
        "        else:\n",
        "            pro_df_r_ls, pro_df_ls = self.import_data(dir_=dir_, roll=True, roll_par=60, ext_1=ext_1, ext_2=\".csv\",\n",
        "                                                      pro_=pro_)\n",
        "            pro_df_ls, _ = self.normalization(pro_df_ls=pro_df_ls, init_=init_, mean_=norm_[0], sigma_=norm_[1])\n",
        "\n",
        "        return pro_df_r_ls, pro_df_ls, norm_\n",
        "\n",
        "    \"\"\" Spliting function \"\"\"\n",
        "\n",
        "    def get_train_test(self, df_list):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            df_list: a list of dataframes (n_df, df)\n",
        "            train_: a number corresponding to the train part\n",
        "        Output:\n",
        "            df_list_train: a training list of dataframes (n_df, df_rows * train_)\n",
        "            df_list_test: a testing list of dataframes (n_df, df_rows * (1 - train_))\n",
        "        \"\"\"\n",
        "\n",
        "        # a number rows in the array:\n",
        "        n_rows = len(df_list[0])\n",
        "\n",
        "        # a number rows for train:\n",
        "        train_n = int(n_rows * self.train_)\n",
        "        val_n = train_n + int(n_rows * self.valid_)\n",
        "        test_n = val_n + int(n_rows * self.test_)\n",
        "\n",
        "        df_ls_train, df_ls_val, df_ls_test = [], [], []\n",
        "        for df_ in df_list:\n",
        "            # split to train and test:\n",
        "            df_train = df_.iloc[:train_n]\n",
        "            df_val = df_.iloc[train_n:val_n]\n",
        "            df_test = df_.iloc[val_n:test_n]\n",
        "\n",
        "            df_ls_train.append(df_train)\n",
        "            df_ls_val.append(df_val)\n",
        "            df_ls_test.append(df_test)\n",
        "\n",
        "        return df_ls_train, df_ls_val, df_ls_test\n",
        "\n",
        "    \"\"\" Well positions matrix function \"\"\"\n",
        "\n",
        "    def get_well_position(self, df_list, init_=True):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            df_list: a list of dataframes (n_df, df)\n",
        "        Output:\n",
        "            well_position: a numpy array of well positions (n_df, n_coor)\n",
        "        \"\"\"\n",
        "\n",
        "        # an empty list:\n",
        "        well_position = []\n",
        "\n",
        "        # loop trough the dataframes and get i-th and j-th coordinates:\n",
        "        if init_:\n",
        "            for df_ in df_list:\n",
        "                well_position.append([df_['i'].values[0], df_['j'].values[0]])\n",
        "        else:\n",
        "            well_position = [df_list['i'].values[0], df_list['j'].values[0]]\n",
        "        # convert to numpy array\n",
        "        return np.array(well_position)\n",
        "\n",
        "    \"\"\" Weighting distance matrix function \"\"\"\n",
        "\n",
        "    def get_weight(self, well_position, empty=False):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            well_position: a numpy array of a well positions (n_df, n_coor)\n",
        "            eps_: an addition to remove 0 values from denominator (always=1)\n",
        "            factor_: a weighting factor\n",
        "        Output:\n",
        "            weight_matrix: a numpy array of weighting distance (n_df, n_df)\n",
        "        \"\"\"\n",
        "\n",
        "        # a number of wells:\n",
        "        pro_n = len(well_position)\n",
        "\n",
        "        # an empty list:\n",
        "        distance_ = []\n",
        "\n",
        "        # loop trough the dataframes:\n",
        "        for i in range(pro_n):\n",
        "\n",
        "            # an empty list for i-th dataframe:\n",
        "            distance_j = []\n",
        "\n",
        "            # loop trough the dataframes:\n",
        "            for j in range(pro_n):\n",
        "                # calculate the distance between i-th and j-th well\n",
        "                distance = np.linalg.norm(well_position[i, :] - well_position[j, :])\n",
        "                distance_j.append(distance)\n",
        "            distance_.append(distance_j)\n",
        "\n",
        "        # stack the i-th distances vectors\n",
        "        distance_ = np.vstack(distance_)\n",
        "\n",
        "        # distance for the current wells:\n",
        "        distance_[distance_ == 0] = 1\n",
        "\n",
        "        # compute the distance weighting matrix\n",
        "        weight_matrix = 1 / distance_ ** self.factor_\n",
        "\n",
        "        if empty:\n",
        "            # weights for the other wells:\n",
        "            weight_matrix[weight_matrix != 1] = 0\n",
        "\n",
        "        return weight_matrix\n",
        "\n",
        "    \"\"\" Values in numpy array function \"\"\"\n",
        "\n",
        "    def get_val_in_arr(self, pro_df_ls, col_):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            df_list: a list of dataframes (n_df, df)\n",
        "            col_: a name of the column (string)\n",
        "        Output:\n",
        "            df_values: a numpy array of values for given column (n_df, df_rows)\n",
        "        \"\"\"\n",
        "\n",
        "        # an empty list:\n",
        "        df_values = []\n",
        "\n",
        "        # loop trough the dataframes:\n",
        "        for df_ in pro_df_ls:\n",
        "            # get the values from given column name:\n",
        "            df_values.append(df_[col_].values)\n",
        "\n",
        "        # return a numpy array of a list\n",
        "        return np.array(df_values)\n",
        "\n",
        "    \"\"\" Weighted value arrays function \"\"\"\n",
        "\n",
        "    def get_weigthed_arr(self, arr_, weight_matrix):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            arr_: a numpy array of the values for given variable (n_df, df_rows)\n",
        "            weight_matrix: a numpy array of weighting distance (n_df, n_df)\n",
        "        Output:\n",
        "            arr_w: a numpy array of the weighted values for given variable (n_df, df_rows, n_df)\n",
        "        \"\"\"\n",
        "\n",
        "        # a number of wells:\n",
        "        pro_n = np.shape(arr_)[0]\n",
        "\n",
        "        # an empty list:\n",
        "        arr_w = []\n",
        "\n",
        "        # loop trough the dataframes:\n",
        "        for i in range(pro_n):\n",
        "            # multiply the values by the weight distance matrix\n",
        "            arr_w.append(np.array(arr_).T * np.atleast_2d(weight_matrix[i, :]))\n",
        "\n",
        "        # return a numpy array of a list\n",
        "        return np.array(arr_w)\n",
        "\n",
        "    \"\"\" Compute gradient function \"\"\"\n",
        "\n",
        "    def get_grad(self, var_ls):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            var_ls: the list of variables in numpy array\n",
        "        Output:\n",
        "            var_grad_ls: the list of gradients in numpy array\n",
        "        \"\"\"\n",
        "\n",
        "        # an empty list:\n",
        "        var_grad_ls = []\n",
        "\n",
        "        # loop trough the list\n",
        "        for var_ in var_ls:\n",
        "            var_grad = var_[:, 1:-1] - var_[:, :-2]\n",
        "            # add first two gradient for correctness\n",
        "            var_grad = np.append(var_grad[:, :2], var_grad, axis=1)\n",
        "            var_grad_ls.append(var_grad)\n",
        "\n",
        "        return var_grad_ls\n",
        "\n",
        "    \"\"\" Pipeline initialization function \"\"\"\n",
        "\n",
        "    def pipeline_init(self, df_list, weight_matrix, init_=True):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            df_list:\n",
        "            col_ls:\n",
        "            train_:\n",
        "        Output:\n",
        "            var_ls_train:\n",
        "            var_ls_test:\n",
        "            const_ls_train:\n",
        "            const_ls_test:\n",
        "            weight_matrix:\n",
        "        \"\"\"\n",
        "\n",
        "        # import function:\n",
        "        pro_df_ls_train, pro_df_ls_val, pro_df_ls_test = self.get_train_test(df_list=df_list)\n",
        "        print(\"Data are imported.\")\n",
        "\n",
        "        # well positions matrix function:\n",
        "        well_position = self.get_well_position(df_list=pro_df_ls_train)\n",
        "\n",
        "        # weighting distance matrix function:\n",
        "        if init_:\n",
        "            weight_matrix = self.get_weight(well_position)\n",
        "\n",
        "        # values in numpy array function:\n",
        "        rate_train_ls, rate_val_ls, rate_test_ls = [], [], []\n",
        "        for col_ in self.col_ls:\n",
        "            rate_train = self.get_val_in_arr(pro_df_ls=pro_df_ls_train, col_=col_)\n",
        "            rate_val = self.get_val_in_arr(pro_df_ls=pro_df_ls_val, col_=col_)\n",
        "            rate_test = self.get_val_in_arr(pro_df_ls=pro_df_ls_test, col_=col_)\n",
        "            rate_train_ls.append(rate_train)\n",
        "            rate_val_ls.append(rate_val)\n",
        "            rate_test_ls.append(rate_test)\n",
        "        time_train = self.get_val_in_arr(pro_df_ls=pro_df_ls_train, col_=self.col_time)\n",
        "        time_val = self.get_val_in_arr(pro_df_ls=pro_df_ls_val, col_=self.col_time)\n",
        "        time_test = self.get_val_in_arr(pro_df_ls=pro_df_ls_test, col_=self.col_time)\n",
        "\n",
        "        # get coordinate values:\n",
        "        coor_train_ls, coor_val_ls, coor_test_ls = [], [], []\n",
        "        for col_ in self.col_coor:\n",
        "            coor_train = self.get_val_in_arr(pro_df_ls=pro_df_ls_train, col_=col_)\n",
        "            coor_val = self.get_val_in_arr(pro_df_ls=pro_df_ls_val, col_=col_)\n",
        "            coor_test = self.get_val_in_arr(pro_df_ls=pro_df_ls_test, col_=col_)\n",
        "            coor_train_ls.append(coor_train)\n",
        "            coor_val_ls.append(coor_val)\n",
        "            coor_test_ls.append(coor_test)\n",
        "\n",
        "        # get gradient values:\n",
        "        grad_train_ls = self.get_grad(var_ls=rate_train_ls[0:3])\n",
        "        grad_val_ls = self.get_grad(var_ls=rate_val_ls[0:3])\n",
        "        grad_test_ls = self.get_grad(var_ls=rate_test_ls[0:3])\n",
        "\n",
        "        # weighted value arrays function:\n",
        "        rate_w_train_ls, rate_w_val_ls, rate_w_test_ls = [], [], []\n",
        "        grad_w_train_ls, grad_w_val_ls, grad_w_test_ls = [], [], []\n",
        "        for i in range(len(rate_train_ls)):\n",
        "            rate_w_train = self.get_weigthed_arr(arr_=rate_train_ls[i], weight_matrix=weight_matrix)\n",
        "            rate_w_val = self.get_weigthed_arr(arr_=rate_val_ls[i], weight_matrix=weight_matrix)\n",
        "            rate_w_test = self.get_weigthed_arr(arr_=rate_test_ls[i], weight_matrix=weight_matrix)\n",
        "            rate_w_train_ls.append(rate_w_train)\n",
        "            rate_w_val_ls.append(rate_w_val)\n",
        "            rate_w_test_ls.append(rate_w_test)\n",
        "        for i in range(len(grad_train_ls)):\n",
        "            grad_w_train = self.get_weigthed_arr(arr_=grad_train_ls[i], weight_matrix=weight_matrix)\n",
        "            grad_w_val = self.get_weigthed_arr(arr_=grad_val_ls[i], weight_matrix=weight_matrix)\n",
        "            grad_w_test = self.get_weigthed_arr(arr_=grad_test_ls[i], weight_matrix=weight_matrix)\n",
        "            grad_w_train_ls.append(grad_w_train)\n",
        "            grad_w_val_ls.append(grad_w_val)\n",
        "            grad_w_test_ls.append(grad_w_test)\n",
        "\n",
        "        # split values to variables and constants:\n",
        "        var_ls_train, var_ls_val, var_ls_test = [], [], []\n",
        "        const_ls_train, const_ls_val, const_ls_test = [], [], []\n",
        "        for i in range(len(self.col_ls)):\n",
        "            if self.col_ls[i] in self.var_col:\n",
        "                var_ls_train.append(rate_w_train_ls[i])\n",
        "                var_ls_val.append(rate_w_val_ls[i])\n",
        "                var_ls_test.append(rate_w_test_ls[i])\n",
        "            else:\n",
        "                const_ls_train.append(rate_w_train_ls[i])\n",
        "                const_ls_val.append(rate_w_val_ls[i])\n",
        "                const_ls_test.append(rate_w_test_ls[i])\n",
        "        print(\"Date are separated to variables, gradients, constants, coordinates, and time.\")\n",
        "\n",
        "        if init_:\n",
        "            return var_ls_train, var_ls_test, const_ls_train, const_ls_test, weight_matrix, \\\n",
        "                   time_train, time_test, grad_w_train_ls, grad_w_test_ls, coor_train_ls, coor_test_ls, \\\n",
        "                   var_ls_val, const_ls_val, time_val, grad_w_val_ls, coor_val_ls\n",
        "        else:\n",
        "            return var_ls_test, const_ls_test, time_test, grad_w_test_ls, coor_test_ls\n",
        "\n",
        "    \"\"\" Splitting to wells number function \"\"\"\n",
        "\n",
        "    def split_data(self, var_w_ls_, const_w_ls_, time_, grad_w_ls_, coor_ls_, title_=\"training\"):\n",
        "\n",
        "        var_split = []\n",
        "        grad_split = []\n",
        "        target_split = []\n",
        "        time_split = []\n",
        "        for i in range(self.n_well):\n",
        "            var_i_ = []\n",
        "            grad_i_ = []\n",
        "            target_i_ = []\n",
        "            for j in range(len(self.var_col)):\n",
        "                var_i_j = var_w_ls_[j][i, :, :]\n",
        "                grad_i_j = grad_w_ls_[j][i, :, :]\n",
        "                target_i_j = var_w_ls_[j][i, :, i]\n",
        "                var_i_.append(var_i_j)\n",
        "                grad_i_.append(grad_i_j)\n",
        "                target_i_.append(target_i_j)\n",
        "            var_split.append(var_i_)\n",
        "            grad_split.append(grad_i_)\n",
        "            target_split.append(target_i_)\n",
        "            time_split.append(time_[i, :])\n",
        "\n",
        "        const_split = []\n",
        "        for i in range(self.n_well):\n",
        "            const_i_ = []\n",
        "            for j in range(len(self.const_col)):\n",
        "                const_i_j = const_w_ls_[j][i, :, :]\n",
        "                const_i_.append(const_i_j)\n",
        "            const_split.append(const_i_)\n",
        "\n",
        "        coor_split = []\n",
        "        for i in range(self.n_well):\n",
        "            coor_i_ = []\n",
        "            for j in range(len(self.col_coor)):\n",
        "                coor_i_j = coor_ls_[j][i, :]\n",
        "                coor_i_.append(coor_i_j)\n",
        "            coor_split.append(coor_i_)\n",
        "        print(\"Date are splitted to each well for \" + str(title_) + \" dataset.\")\n",
        "        return var_split, const_split, time_split, grad_split, coor_split, target_split\n",
        "\n",
        "    def connect_data(self, var_o, const_o, time_o, grad_o, coor_o, target_o,\n",
        "                     var_n, const_n, time_n, grad_n, coor_n, target_n):\n",
        "\n",
        "        var_con, const_con, time_con, grad_con, coor_con, target_con = [], [], [], [], [], []\n",
        "\n",
        "        for well_ in range(self.n_well):\n",
        "            var_con_i, const_con_i, grad_con_i, coor_con_i, target_con_i \\\n",
        "                = [], [], [], [], []\n",
        "\n",
        "            time_con_i = np.append(time_o[well_], time_n[well_], axis=0)\n",
        "            for i in range(len(self.var_col)):\n",
        "                var_con_i_well = np.append(var_o[well_][i], var_n[well_][i], axis=0)\n",
        "                var_con_i.append(var_con_i_well)\n",
        "                grad_con_i_well = np.append(grad_o[well_][i], grad_n[well_][i], axis=0)\n",
        "                grad_con_i.append(grad_con_i_well)\n",
        "                target_con_i_well = np.append(target_o[well_][i], target_n[well_][i], axis=0)\n",
        "                target_con_i.append(target_con_i_well)\n",
        "            for i in range(len(self.const_col)):\n",
        "                const_con_i_well = np.append(const_o[well_][i], const_n[well_][i], axis=0)\n",
        "                const_con_i.append(const_con_i_well)\n",
        "            for i in range(len(self.col_coor)):\n",
        "                coor_con_i_well = np.append(coor_o[well_][i], coor_n[well_][i], axis=0)\n",
        "                coor_con_i.append(coor_con_i_well)\n",
        "            var_con.append(var_con_i)\n",
        "            const_con.append(const_con_i)\n",
        "            time_con.append(time_con_i)\n",
        "            grad_con.append(grad_con_i)\n",
        "            coor_con.append(coor_con_i)\n",
        "            target_con.append(target_con_i)\n",
        "\n",
        "        print(\"Provided data are connected\")\n",
        "        return var_con, const_con, time_con, grad_con, coor_con, target_con\n",
        "\n",
        "    def get_data_target(self, var_, const_, time_, grad_, coor_, target_, bias_=None):\n",
        "        target_arr = np.array(target_)[:, self.n_old_obs:]\n",
        "        for old_ in range(self.n_old_obs):\n",
        "            if old_ == 0:\n",
        "                obs_arr_ = var_[0][old_:-(self.n_old_obs - old_), :]\n",
        "                for i in range(1, len(self.var_col)):\n",
        "                    obs_arr_ = np.append(obs_arr_, var_[i][old_:-(self.n_old_obs - old_), :], axis=1)\n",
        "            else:\n",
        "                for i in range(len(self.var_col)):\n",
        "                    obs_arr_ = np.append(obs_arr_, var_[i][old_:-(self.n_old_obs - old_), :], axis=1)\n",
        "            if self.sec_order:\n",
        "                for i in range(len(self.var_col)):\n",
        "                    obs_arr_ = np.append(obs_arr_, np.power(var_[i][old_:-(self.n_old_obs - old_), :], 2), axis=1)\n",
        "            for i in range(len(self.var_col)):\n",
        "                obs_arr_ = np.append(obs_arr_, grad_[i][old_:-(self.n_old_obs - old_), :], axis=1)\n",
        "            for i in range(len(self.const_col)):\n",
        "                obs_arr_ = np.append(obs_arr_, const_[i][old_:-(self.n_old_obs - old_), :], axis=1)\n",
        "            for i in range(len(self.col_coor)):\n",
        "                obs_arr_ = np.append(obs_arr_, np.atleast_2d(coor_[i][old_:-(self.n_old_obs - old_)]).T, axis=1)\n",
        "        obs_arr_ = np.append(obs_arr_, np.atleast_2d(time_[self.n_old_obs:]).T, axis=1)\n",
        "        if bias_ is None:\n",
        "            bias_ = np.ones((np.shape(obs_arr_)[0], 1))\n",
        "        obs_arr_ = np.append(obs_arr_, bias_, axis=1)\n",
        "\n",
        "        return obs_arr_, target_arr\n",
        "\n",
        "    def get_starters(self, var_, const_, time_, grad_, coor_, target_):\n",
        "        target_arr = [ta_[1:self.n_old_obs + 3] for ta_ in target_]\n",
        "        var_arr_ = [np.atleast_2d(v_[:self.n_old_obs + 3, :]) for v_ in var_]\n",
        "        grad_arr_ = [np.atleast_2d(g_[:self.n_old_obs + 3, :]) for g_ in grad_]\n",
        "        const_arr_ = [np.atleast_2d(c_[:self.n_old_obs + 3, :]) for c_ in const_]\n",
        "        coor_arr_ = [co_[:self.n_old_obs + 3] for co_ in coor_]\n",
        "        time_arr_ = time_[:self.n_old_obs + 3]\n",
        "        return var_arr_, const_arr_, time_arr_, grad_arr_, coor_arr_, target_arr\n",
        "\n",
        "    def get_endings(self, var_, const_, time_, grad_, coor_, target_, minus_start=0):\n",
        "        target_arr = [ta_[-(self.n_old_obs + 3 + minus_start):] for ta_ in target_]\n",
        "        var_arr_ = [np.atleast_2d(v_[-(self.n_old_obs + 3 + minus_start):, :]) for v_ in var_]\n",
        "        grad_arr_ = [np.atleast_2d(g_[-(self.n_old_obs + 3 + minus_start):, :]) for g_ in grad_]\n",
        "        const_arr_ = [np.atleast_2d(c_[-(self.n_old_obs + 3 + minus_start):, :]) for c_ in const_]\n",
        "        coor_arr_ = [co_[-(self.n_old_obs + 3 + minus_start):] for co_ in coor_]\n",
        "        time_arr_ = time_[-(self.n_old_obs + 3 + minus_start):]\n",
        "        return var_arr_, const_arr_, time_arr_, grad_arr_, coor_arr_, target_arr\n",
        "\n",
        "    \"\"\" Pipeline modeling for splited data function \"\"\"\n",
        "\n",
        "    def pipeline_model_split(self, var_split, const_split, time_split, grad_split,\n",
        "                             coor_split, target_split, plot_=False):\n",
        "\n",
        "        # observation matrix creation function:\n",
        "        a_ls = []\n",
        "        l_ls = []\n",
        "        model_ls = []\n",
        "        l_pred_ls = []\n",
        "        rmse_ls_all, std_ls_all = [], []\n",
        "        score_ls_all, score_ls_all_ls = [], []\n",
        "        for i in range(self.n_well):\n",
        "            a_i_, l_i = self.get_data_target(var_=var_split[i], const_=const_split[i], time_=time_split[i],\n",
        "                                             grad_=grad_split[i], coor_=coor_split[i], target_=target_split[i])\n",
        "            a_ls.append(a_i_)\n",
        "            l_ls.append(l_i)\n",
        "            score_ls, rmse_ls, std_ls = [], [], []\n",
        "\n",
        "            if self.lin_:\n",
        "                self.lin_model()\n",
        "                lin_model = self.model_\n",
        "                model_ls_var = MultiOutputRegressor(estimator=lin_model)\n",
        "                model_ls_var.fit(a_i_, l_i.T)\n",
        "                l_i_pred = model_ls_var.predict(a_i_)\n",
        "                for var_ in range(len(self.var_col)):\n",
        "                    score_, rmse_, std_ = self.model_score(y_=l_i[var_], y_pred_=l_i_pred.T[var_])\n",
        "                    score_ls.append(score_)\n",
        "                    rmse_ls.append(rmse_)\n",
        "                    std_ls.append(std_)\n",
        "\n",
        "            if self.lstm_ or self.dense_:\n",
        "                if self.dense_:\n",
        "                    self.dense_model()\n",
        "                elif self.lstm_:\n",
        "                    self.lstm_model()\n",
        "                a_conv, a_rs_conv, l_ls_conv = self.make_dataset_ts(data_train=a_i_, data_target=l_i)\n",
        "                print(\"Dataset for well no. \" + str(i + 1) + \" is prepared for keras.\")\n",
        "                model_ls_var = []\n",
        "                l_i_pred = []\n",
        "                for var_ in range(len(self.var_col)):\n",
        "                    model_var = self.model_\n",
        "                    his_ = self.compile_and_fit(model=model_var, data=(a_rs_conv, l_ls_conv[var_]))\n",
        "\n",
        "                    # Temporary save the model\n",
        "                    mod_temp_dir_w = self.mod_temp_dir + \"Well_no_\" + str(i + 1)\n",
        "                    if not os.path.exists(mod_temp_dir_w):\n",
        "                        os.mkdir(mod_temp_dir_w)\n",
        "                    mod_temp_dir_w = mod_temp_dir_w + \"/Model_\" + str(self.var_col[var_])\n",
        "                    model_var.save(mod_temp_dir_w)\n",
        "                    l_i_pred_var = model_var.predict(a_rs_conv)\n",
        "                    l_i_pred_var = l_i_pred_var[:, 0, 0]\n",
        "                    score_, rmse_, std_ = self.model_score(y_=l_ls_conv[var_], y_pred_=l_i_pred_var)\n",
        "                    score_ls.append(score_)\n",
        "                    rmse_ls.append(rmse_)\n",
        "                    std_ls.append(std_)\n",
        "                    l_i_pred.append(l_i_pred_var)\n",
        "                    model_ls_var.append(model_var)\n",
        "\n",
        "            score_ls_avg = np.mean(np.array(score_ls))\n",
        "            model_ls.append(model_ls_var)\n",
        "            score_ls_all.append(score_ls_avg)\n",
        "            score_ls_all_ls.append(score_ls)\n",
        "            l_pred_ls.append(l_i_pred)\n",
        "            rmse_ls_all.append(rmse_ls)\n",
        "            std_ls_all.append(std_ls)\n",
        "            if plot_:\n",
        "                for var_ in range(len(self.var_col)):\n",
        "                    if self.dense_ or self.lstm_:\n",
        "                        y_pred_ = l_i_pred[var_]\n",
        "                    elif self.lin_:\n",
        "                        y_pred_ = l_i_pred.T[var_]\n",
        "                    x_ = time_split[i][self.n_old_obs:]\n",
        "                    y_ = l_i[var_]\n",
        "                    title_ = \"Prediction for well no. \" + str(i + 1) + \" \" + str(self.var_col[var_])\n",
        "                    self.get_plot(x_=x_, y_=y_, y_pred=y_pred_, y_label=title_)\n",
        "                    print(\"Model for well number \" + str(i + 1) + \" and \" + str(self.var_col[var_]) +\n",
        "                          \" is fitted with RMSE: \" + str(\"{:.4f}\".format(rmse_ls[var_])) + \".\")\n",
        "            print(\"Model for well number \" + str(i + 1) + \" is fitted with average error: \" + str(\n",
        "                \"{:.4f}\".format(score_ls_avg)) + \"%.\")\n",
        "        print(\"Model is fitted with error: \" + str(\"{:.4f}\".format(np.mean(np.array(score_ls_all)))) + \"%.\")\n",
        "        time_ax = time_split[0][self.n_old_obs:]\n",
        "        return a_ls, l_ls, l_pred_ls, model_ls, score_ls_all_ls, rmse_ls_all, std_ls_all, time_ax\n",
        "\n",
        "    def get_prediction(self, model_, var_split, const_split, time_split, grad_split,\n",
        "                       coor_split, target_split, plot_=False, new_well=False):\n",
        "\n",
        "        avg_score, l_ls, l_pred_ls = [], [], []\n",
        "        rmse_ls_all, std_ls_all = [], []\n",
        "        for well_ in range(self.n_well):\n",
        "            a_i_, l_i = self.get_data_target(var_=var_split[well_], const_=const_split[well_], time_=time_split[well_],\n",
        "                                             grad_=grad_split[well_], coor_=coor_split[well_],\n",
        "                                             target_=target_split[well_])\n",
        "            l_ls.append(l_i)\n",
        "            avg_score_well, rmse_well, std_well = [], [], []\n",
        "            if self.lin_:\n",
        "                l_pred = model_[well_].predict(a_i_)\n",
        "                l_i_pred = []\n",
        "                for var_ in range(len(self.var_col)):\n",
        "                    x_ = time_split[well_][self.n_old_obs:]\n",
        "                    y_ = l_i[var_]\n",
        "                    l_i_pred_var = l_pred.T[var_]\n",
        "                    avg_score_well_var, rmse_well_var, std_well_var = self.model_score(y_=y_, y_pred_=l_i_pred_var)\n",
        "                    avg_score_well.append(avg_score_well_var)\n",
        "                    rmse_well.append(rmse_well_var)\n",
        "                    std_well.append(std_well_var)\n",
        "                    l_i_pred.append(l_i_pred_var)\n",
        "                    if plot_:\n",
        "                        title_ = \"Prediction for well no. \" + str(well_ + 1) + \" \" + str(self.var_col[var_])\n",
        "                        self.get_plot(x_=x_, y_=y_, y_pred=l_i_pred_var, y_label=title_)\n",
        "                        if not new_well:\n",
        "                            print(\"Model for well number \" + str(well_ + 1) + \" and \" + str(self.var_col[var_]) +\n",
        "                                  \" is fitted with RMSE: \" + str(\"{:.4f}\".format(rmse_well_var)) + \".\")\n",
        "\n",
        "            if self.lstm_ or self.dense_:\n",
        "                a_conv, a_rs_conv, l_ls_conv = self.make_dataset_ts(data_train=a_i_, data_target=l_i)\n",
        "                print(\"Dataset for well no. \" + str(well_ + 1) + \" is prepared for keras.\")\n",
        "                l_i_pred = []\n",
        "                for var_ in range(len(self.var_col)):\n",
        "                    x_ = time_split[well_][self.n_old_obs:]\n",
        "                    y_ = l_ls_conv[var_]\n",
        "                    # Load the temporary model\n",
        "                    mod_temp_dir_w = self.mod_temp_dir + \"Well_no_\" + str(well_ + 1)\n",
        "                    mod_temp_dir_w = mod_temp_dir_w + \"/Model_\" + str(self.var_col[var_])\n",
        "                    model_ = keras.models.load_model(mod_temp_dir_w)\n",
        "\n",
        "                    l_i_pred_var = model_.predict(a_rs_conv)\n",
        "                    del model_\n",
        "                    l_i_pred_var = l_i_pred_var[:, 0, 0]\n",
        "                    avg_score_well_var, rmse_well_var, std_well_var = self.model_score(y_=y_, y_pred_=l_i_pred_var)\n",
        "                    avg_score_well.append(avg_score_well_var)\n",
        "                    rmse_well.append(rmse_well_var)\n",
        "                    std_well.append(std_well_var)\n",
        "                    l_i_pred.append(l_i_pred_var)\n",
        "                    if plot_:\n",
        "                        title_ = \"Prediction for well no. \" + str(well_ + 1) + \" \" + str(self.var_col[var_])\n",
        "                        self.get_plot(x_=x_, y_=y_, y_pred=l_i_pred_var, y_label=title_)\n",
        "                        if not new_well:\n",
        "                            print(\"Model for well number \" + str(well_ + 1) + \" and \" + str(self.var_col[var_]) +\n",
        "                                  \" is fitted with RMSE: \" + str(\"{:.4f}\".format(rmse_well_var)) + \".\")\n",
        "            l_pred_ls.append(l_i_pred)\n",
        "            if not new_well:\n",
        "                print(\"The error for model for well number \" + str(well_ + 1)\n",
        "                      + \": \" + str(\"{:.4f}\".format(np.mean(np.array(avg_score_well)))) + \"%\")\n",
        "\n",
        "            avg_score.append(avg_score_well)\n",
        "            rmse_ls_all.append(rmse_well)\n",
        "            std_ls_all.append(std_well)\n",
        "        avg_score_tot = [np.sum(np.array(arr_)) for arr_ in avg_score]\n",
        "        avg_score_t = np.sum(avg_score_tot) / (self.n_well + len(self.var_col))\n",
        "        if not new_well:\n",
        "            print(\"Average error for the prediction for all variables and wells: \"\n",
        "                  + str(\"{:.4}\".format(avg_score_t)) + \"%\")\n",
        "        time_ax = time_split[0][self.n_old_obs:]\n",
        "        return l_ls, l_pred_ls, avg_score, rmse_ls_all, std_ls_all, time_ax\n",
        "\n",
        "    def get_linear_valid_model(self, weight_matrix, var_train, const_train, time_train,\n",
        "                               grad_train, coor_train, target_train, var_val, const_val,\n",
        "                               time_val, grad_val, coor_val, target_val):\n",
        "        n_old_obs_ls = [15, 20, 25, 30]\n",
        "        mod_update_ls = [50, 100, 200]\n",
        "        w_model_init_ls = [0.00, 0.5, 1.00]\n",
        "\n",
        "        print(\"------------------------------\")\n",
        "        print(\"Model validation for the number of old observations is started\")\n",
        "\n",
        "        avg_score_old_obs_ls = []\n",
        "        for old_obs in n_old_obs_ls:\n",
        "            self.n_old_obs = old_obs\n",
        "            print(\"------------------------------\")\n",
        "            print(\"Model for the number of the old observation: \" + str(old_obs))\n",
        "            a_ls, l_ls, l_pred_ls, model_ls, _, _, _, _ = self.pipeline_model_split(var_split=var_train,\n",
        "                                                                                    const_split=const_train,\n",
        "                                                                                    time_split=time_train,\n",
        "                                                                                    grad_split=grad_train,\n",
        "                                                                                    coor_split=coor_train,\n",
        "                                                                                    target_split=target_train)\n",
        "            n_iter = np.shape(time_val)[1] - (self.n_old_obs + 3)\n",
        "            avg_score_old_obs, _, _, _, _, _, _ = self.get_forecast_split(model_=model_ls,\n",
        "                                                                          weight_matrix=weight_matrix,\n",
        "                                                                          var_split=var_val,\n",
        "                                                                          const_split=const_val,\n",
        "                                                                          time_split=time_val,\n",
        "                                                                          grad_split=grad_val,\n",
        "                                                                          coor_split=coor_val,\n",
        "                                                                          target_split=target_val,\n",
        "                                                                          n_iter=n_iter,\n",
        "                                                                          plot_=False)\n",
        "            avg_score_old_obs_ls.append(avg_score_old_obs)\n",
        "\n",
        "        n_old_obs_idx = np.argmin(avg_score_old_obs_ls)\n",
        "        n_old_obs_best = n_old_obs_ls[n_old_obs_idx]\n",
        "        self.n_old_obs = n_old_obs_best\n",
        "        print(\"Best number of the old observations:\")\n",
        "        print(n_old_obs_best)\n",
        "\n",
        "        print(\"------------------------------\")\n",
        "        print(\"Model validation for the number of the iterations of the model update is started\")\n",
        "\n",
        "        avg_score_mod_update_ls = []\n",
        "        for mod_up in mod_update_ls:\n",
        "            self.mod_update = mod_up\n",
        "            print(\"------------------------------\")\n",
        "            print(\"Model for model update:\")\n",
        "            print(mod_up)\n",
        "            a_ls, l_ls, l_pred_ls, model_ls, _, _, _, _ = self.pipeline_model_split(var_split=var_train,\n",
        "                                                                                    const_split=const_train,\n",
        "                                                                                    time_split=time_train,\n",
        "                                                                                    grad_split=grad_train,\n",
        "                                                                                    coor_split=coor_train,\n",
        "                                                                                    target_split=target_train)\n",
        "            n_iter = np.shape(time_val)[1] - (self.n_old_obs + 3)\n",
        "            avg_score_mod_up, _, _, _, _, _, _ = self.get_forecast_split(model_=model_ls,\n",
        "                                                                         weight_matrix=weight_matrix,\n",
        "                                                                         var_split=var_val,\n",
        "                                                                         const_split=const_val,\n",
        "                                                                         time_split=time_val,\n",
        "                                                                         grad_split=grad_val,\n",
        "                                                                         coor_split=coor_val,\n",
        "                                                                         target_split=target_val,\n",
        "                                                                         n_iter=n_iter,\n",
        "                                                                         plot_=False)\n",
        "            avg_score_mod_update_ls.append(avg_score_mod_up)\n",
        "\n",
        "        mod_update_idx = np.argmin(avg_score_mod_update_ls)\n",
        "        mod_update_best = mod_update_ls[mod_update_idx]\n",
        "        self.mod_update = mod_update_best\n",
        "        print(\"Best number of the iterations for model update:\")\n",
        "        print(mod_update_best)\n",
        "\n",
        "        print(\"------------------------------\")\n",
        "        print(\"Model validation for the weight of the initial model is started\")\n",
        "\n",
        "        avg_score_w_model_init_ls = []\n",
        "        for w_mod in w_model_init_ls:\n",
        "            self.w_model_init = w_mod\n",
        "            print(\"------------------------------\")\n",
        "            print(\"Model for initial model weight:\")\n",
        "            print(w_mod)\n",
        "            a_ls, l_ls, l_pred_ls, model_ls, _, _, _, _ = self.pipeline_model_split(var_split=var_train,\n",
        "                                                                                    const_split=const_train,\n",
        "                                                                                    time_split=time_train,\n",
        "                                                                                    grad_split=grad_train,\n",
        "                                                                                    coor_split=coor_train,\n",
        "                                                                                    target_split=target_train)\n",
        "            n_iter = np.shape(time_val)[1] - (self.n_old_obs + 3)\n",
        "            avg_score_w_mod, _, _, _, _, _, _ = self.get_forecast_split(model_=model_ls,\n",
        "                                                                        weight_matrix=weight_matrix,\n",
        "                                                                        var_split=var_val,\n",
        "                                                                        const_split=const_val,\n",
        "                                                                        time_split=time_val,\n",
        "                                                                        grad_split=grad_val,\n",
        "                                                                        coor_split=coor_val,\n",
        "                                                                        target_split=target_val,\n",
        "                                                                        n_iter=n_iter,\n",
        "                                                                        plot_=False)\n",
        "            avg_score_w_model_init_ls.append(avg_score_w_mod)\n",
        "\n",
        "        w_model_init_idx = np.argmin(avg_score_w_model_init_ls)\n",
        "        w_model_init_best = w_model_init_ls[w_model_init_idx]\n",
        "        self.w_model_init = w_model_init_best\n",
        "        print(\"Best weight of the initial model:\")\n",
        "        print(w_model_init_best)\n",
        "        return n_old_obs_best, mod_update_best, w_model_init_best\n",
        "\n",
        "    def get_forecast_split(self, model_, weight_matrix, var_split, const_split, time_split,\n",
        "                           grad_split, coor_split, target_split, n_iter=None, plot_=False, new_well=False, plot_w=None):\n",
        "\n",
        "        n_var = len(self.var_col)\n",
        "        iter_process = np.round(n_iter, -1)\n",
        "\n",
        "        l_ls_init_two = []\n",
        "        l_ls_init_three = []\n",
        "        if self.lin_:\n",
        "            model_update_ls = model_\n",
        "        l_ls_pred_two = []\n",
        "        l_ls_pred_three = []\n",
        "        l_ls_pred_four = []\n",
        "\n",
        "        var_arr_ls, const_arr_ls, time_arr_ls, grad_arr_ls, coor_arr_ls, target_arr_ls = [], [], [], [], [], []\n",
        "\n",
        "        for well_ in range(self.n_well):\n",
        "            var_arr_, const_arr_, time_arr_, grad_arr_, coor_arr_, target_arr = \\\n",
        "                self.get_starters(var_=var_split[well_], const_=const_split[well_], time_=time_split[well_],\n",
        "                                  grad_=grad_split[well_], coor_=coor_split[well_], target_=target_split[well_])\n",
        "\n",
        "            var_arr_ls.append(var_arr_)\n",
        "            const_arr_ls.append(const_arr_)\n",
        "            time_arr_ls.append(time_arr_)\n",
        "            grad_arr_ls.append(grad_arr_)\n",
        "            coor_arr_ls.append(coor_arr_)\n",
        "            target_arr_ls.append(target_arr)\n",
        "\n",
        "        tf_models = []\n",
        "        for well_ in range(self.n_well):\n",
        "            a_init, l_init = self.get_data_target(var_=var_arr_ls[well_], const_=const_arr_ls[well_],\n",
        "                                                  time_=time_arr_ls[well_], grad_=grad_arr_ls[well_],\n",
        "                                                  coor_=coor_arr_ls[well_], target_=target_arr_ls[well_],\n",
        "                                                  bias_=None)\n",
        "            target_arr_ls_update = []\n",
        "            if self.lin_:\n",
        "                l_pred_two = model_[well_].predict([a_init[0]]).T\n",
        "                l_pred_three = model_[well_].predict([a_init[1]]).T\n",
        "                l_pred_four = model_[well_].predict([a_init[2]]).T\n",
        "                l_pred_two[l_pred_two < 0] = 0\n",
        "                l_pred_three[l_pred_three < 0] = 0\n",
        "                l_pred_four[l_pred_four < 0] = 0\n",
        "\n",
        "            elif self.dense_ or self.lstm_:\n",
        "                a_conv, a_rs_conv, l_ls_conv = self.make_dataset_ts(data_train=a_init, data_target=l_init)\n",
        "                l_pred_two, l_pred_three, l_pred_four = [], [], []\n",
        "                tf_models_var = []\n",
        "                for var_ in range(len(self.var_col)):\n",
        "                    a_one = np.array([a_rs_conv[0, :, :]])\n",
        "                    a_two = np.array([a_rs_conv[1, :, :]])\n",
        "                    a_three = np.array([a_rs_conv[2, :, :]])\n",
        "\n",
        "                    # Load the temporary model\n",
        "                    mod_temp_dir_w = self.mod_temp_dir + \"Well_no_\" + str(well_ + 1)\n",
        "                    mod_temp_dir_w = mod_temp_dir_w + \"/Model_\" + str(self.var_col[var_])\n",
        "                    model_ = keras.models.load_model(mod_temp_dir_w)\n",
        "                    tf_models_var.append(model_)\n",
        "                    l_pred_two_var = model_.predict(a_one)[:, 0, 0]\n",
        "                    l_pred_three_var = model_.predict(a_two)[:, 0, 0]\n",
        "                    l_pred_four_var = model_.predict(a_three)[:, 0, 0]\n",
        "                    del model_\n",
        "                    l_pred_two_var[l_pred_two_var < 0] = 0\n",
        "                    l_pred_three_var[l_pred_three_var < 0] = 0\n",
        "                    l_pred_four_var[l_pred_four_var < 0] = 0\n",
        "                    l_pred_two.append(l_pred_two_var)\n",
        "                    l_pred_three.append(l_pred_three_var)\n",
        "                    l_pred_four.append(l_pred_four_var)\n",
        "                tf_models.append(tf_models_var)\n",
        "            l_ls_pred_two.append(l_pred_two)\n",
        "            l_ls_pred_three.append(l_pred_three)\n",
        "            l_ls_pred_four.append(l_pred_four)\n",
        "            l_ls_init_two.append(l_init[:, 0])\n",
        "            l_ls_init_three.append(l_init[:, 1])\n",
        "\n",
        "        l_pred_ls_tot_two = np.append(np.atleast_3d(l_ls_init_two), np.atleast_3d(l_ls_init_two), axis=2)\n",
        "        l_pred_ls_tot_three = np.append(np.atleast_3d(l_ls_init_three), np.atleast_3d(l_ls_init_three), axis=2)\n",
        "        l_pred_ls_tot_four = np.append(np.atleast_3d(l_ls_pred_three), np.atleast_3d(l_ls_pred_four), axis=2)\n",
        "\n",
        "        for well_ in range(self.n_well):\n",
        "            well_target_arr_ls_update = []\n",
        "            for var_ in range(n_var):\n",
        "                well_var_target_update = np.append(np.atleast_1d(target_arr_ls[well_][var_][0]),\n",
        "                                                   target_arr_ls[well_][var_][:], axis=0)\n",
        "                well_target_arr_ls_update.append(well_var_target_update)\n",
        "            target_arr_ls_update.append(well_target_arr_ls_update)\n",
        "        target_arr_ls = target_arr_ls_update\n",
        "\n",
        "        for iter_ in range(n_iter):\n",
        "            grad_ = []\n",
        "            new_pred_ = []\n",
        "            # Compute gradient:\n",
        "            for well_ in range(self.n_well):\n",
        "                grad_well, new_pred_well = [], []\n",
        "                for var_ in range(n_var):\n",
        "                    old_pred_well_var = l_pred_ls_tot_two[well_][var_][-2]\n",
        "                    new_pred_well_var = l_pred_ls_tot_three[well_][var_][-1]\n",
        "                    grad_well_var = new_pred_well_var - old_pred_well_var\n",
        "                    new_pred_well_var_true = l_pred_ls_tot_four[well_][var_][-2] + grad_well_var\n",
        "\n",
        "                    grad_well.append(grad_well_var)\n",
        "                    new_pred_well.append(new_pred_well_var_true)\n",
        "                grad_.append(grad_well)\n",
        "                new_pred_.append(new_pred_well)\n",
        "                # Add date:\n",
        "                time_arr_ls[well_] = time_arr_ls[well_] + 1\n",
        "            # Reshape gradient / prediction to (var_n, well_n):\n",
        "            new_pred_order, grad_order = [], []\n",
        "            for var_ in range(n_var):\n",
        "                new_pred_order.append([n_p[var_] for n_p in new_pred_])\n",
        "                grad_order.append([n_g[var_] for n_g in grad_])\n",
        "\n",
        "            # Multiply by weighting matrix:\n",
        "            new_pred_order_w, grad_order_w = [], []\n",
        "            for var_ in range(n_var):\n",
        "                new_pred_order_w.append(new_pred_order[var_] * weight_matrix)\n",
        "                grad_order_w.append(grad_order[var_] * weight_matrix)\n",
        "\n",
        "            # Back to initial shape:\n",
        "            new_pred_w, grad_w = [], []\n",
        "            for well_ in range(self.n_well):\n",
        "                new_pred_w.append([n_p_w[well_] for n_p_w in new_pred_order_w])\n",
        "                grad_w.append([n_g_w[well_] for n_g_w in grad_order_w])\n",
        "\n",
        "            # Add new weighted prediction to the matrix of variables:\n",
        "            var_arr_ls_update, grad_arr_ls_update, target_arr_ls_update = [], [], []\n",
        "\n",
        "            for well_ in range(self.n_well):\n",
        "                well_update, well_grad_update, well_target_update = [], [], []\n",
        "                for var_ in range(n_var):\n",
        "                    well_var_update = np.append(var_arr_ls[well_][var_][1:, :], np.atleast_2d(new_pred_w[well_][var_]),\n",
        "                                                axis=0)\n",
        "\n",
        "                    well_var_grad_update = np.append(grad_arr_ls[well_][var_][1:, :],\n",
        "                                                     np.atleast_2d(grad_w[well_][var_]), axis=0)\n",
        "\n",
        "                    well_var_target_update = np.append(target_arr_ls[well_][var_][1:],\n",
        "                                                       np.atleast_1d(l_pred_ls_tot_four[well_][var_][-1]), axis=0)\n",
        "                    well_update.append(well_var_update)\n",
        "                    well_grad_update.append(well_var_grad_update)\n",
        "                    well_target_update.append(well_var_target_update)\n",
        "                var_arr_ls_update.append(well_update)\n",
        "                grad_arr_ls_update.append(well_grad_update)\n",
        "                target_arr_ls_update.append(well_target_update)\n",
        "\n",
        "            var_arr_ls = var_arr_ls_update\n",
        "            grad_arr_ls = grad_arr_ls_update\n",
        "            target_arr_ls = target_arr_ls_update\n",
        "\n",
        "            # Get new observation matrix and prediction:\n",
        "            l_ls_init_two_update = []\n",
        "            l_ls_init_three_update = []\n",
        "\n",
        "            l_ls_pred_two_update = []\n",
        "            l_ls_pred_three_update = []\n",
        "            l_ls_pred_four_update = []\n",
        "\n",
        "            for well_ in range(self.n_well):\n",
        "                a_update, l_update = self.get_data_target(var_=var_arr_ls[well_], const_=const_arr_ls[well_],\n",
        "                                                          time_=time_arr_ls[well_], grad_=grad_arr_ls[well_],\n",
        "                                                          coor_=coor_arr_ls[well_], target_=target_arr_ls[well_],\n",
        "                                                          bias_=None)\n",
        "\n",
        "                l_ls_init_two_update.append(l_update[:, 0])\n",
        "                l_ls_init_three_update.append(l_update[:, 1])\n",
        "                # Update for n_old_obs:\n",
        "                if self.lin_:\n",
        "                    # Update for Walk-Forward validation:\n",
        "                    if iter_ % self.mod_update == 0:\n",
        "                        model_update_ls[well_].fit(a_update[:-1, :], l_update[:, :-1].T)\n",
        "\n",
        "                    # Basic Trend:\n",
        "                    l_pred_two_update_init = model_[well_].predict([a_update[0]]).T\n",
        "                    l_pred_three_update_init = model_[well_].predict([a_update[1]]).T\n",
        "                    l_pred_four_update_init = model_[well_].predict([a_update[2]]).T\n",
        "\n",
        "                    # Update trend for the forecast:\n",
        "                    l_pred_two_update_new = model_update_ls[well_].predict([a_update[0]]).T\n",
        "                    l_pred_three_update_new = model_update_ls[well_].predict([a_update[1]]).T\n",
        "                    l_pred_four_update_new = model_update_ls[well_].predict([a_update[2]]).T\n",
        "\n",
        "                    l_pred_two_update = (l_pred_two_update_new * self.w_model_update +\n",
        "                                         l_pred_two_update_init * self.w_model_init\n",
        "                                         ) / (self.w_model_update + self.w_model_init)\n",
        "\n",
        "                    l_pred_three_update = (l_pred_three_update_new * self.w_model_update +\n",
        "                                           l_pred_three_update_init * self.w_model_init\n",
        "                                           ) / (self.w_model_update + self.w_model_init)\n",
        "\n",
        "                    l_pred_four_update = (l_pred_four_update_new * self.w_model_update +\n",
        "                                          l_pred_four_update_init * self.w_model_init\n",
        "                                          ) / (self.w_model_update + self.w_model_init)\n",
        "\n",
        "                    l_pred_two_update[l_pred_two_update < 0] = 0\n",
        "                    l_pred_three_update[l_pred_three_update < 0] = 0\n",
        "                    l_pred_four_update[l_pred_four_update < 0] = 0\n",
        "\n",
        "                elif self.dense_ or self.lstm_:\n",
        "                    a_conv, a_rs_conv, l_ls_conv = self.make_dataset_ts(data_train=a_update, data_target=l_update)\n",
        "                    l_pred_two_update, l_pred_three_update, l_pred_four_update = [], [], []\n",
        "                    a_one = np.array([a_rs_conv[0, :, :]])\n",
        "                    a_two = np.array([a_rs_conv[1, :, :]])\n",
        "                    a_three = np.array([a_rs_conv[2, :, :]])\n",
        "                    for var_ in range(len(self.var_col)):\n",
        "\n",
        "                        # Load the temporary model\n",
        "                        mod_temp_dir_w = self.mod_temp_dir + \"Well_no_\" + str(well_ + 1)\n",
        "                        mod_temp_dir_w = mod_temp_dir_w + \"/Model_\" + str(self.var_col[var_])\n",
        "                        #model_ = keras.models.load_model(mod_temp_dir_w)\n",
        "                        model_ = tf_models[well_][var_]\n",
        "                        l_pred_two_update_var_init = model_.predict(a_one)[:, 0, 0]\n",
        "                        l_pred_three_update_var_init = model_.predict(a_two)[:, 0, 0]\n",
        "                        l_pred_four_update_var_init = model_.predict(a_three)[:, 0, 0]\n",
        "                        del model_\n",
        "\n",
        "                        l_pred_two_update_var = l_pred_two_update_var_init\n",
        "                        l_pred_three_update_var = l_pred_three_update_var_init\n",
        "                        l_pred_four_update_var = l_pred_four_update_var_init\n",
        "\n",
        "                        l_pred_two_update_var[l_pred_two_update_var < 0] = 0\n",
        "                        l_pred_three_update_var[l_pred_three_update_var < 0] = 0\n",
        "                        l_pred_four_update_var[l_pred_four_update_var < 0] = 0\n",
        "\n",
        "                        l_pred_two_update.append(l_pred_two_update_var)\n",
        "                        l_pred_three_update.append(l_pred_three_update_var)\n",
        "                        l_pred_four_update.append(l_pred_four_update_var)\n",
        "\n",
        "                l_ls_pred_two_update.append(l_pred_two_update)\n",
        "                l_ls_pred_three_update.append(l_pred_three_update)\n",
        "                l_ls_pred_four_update.append(l_pred_four_update)\n",
        "\n",
        "            l_pred_ls_tot_two = np.append(l_pred_ls_tot_two, np.atleast_3d(l_ls_init_two_update), axis=2)\n",
        "            l_pred_ls_tot_three = np.append(l_pred_ls_tot_three, np.atleast_3d(l_ls_init_three_update), axis=2)\n",
        "            l_pred_ls_tot_four = np.append(l_pred_ls_tot_four, np.atleast_3d(l_ls_pred_four_update), axis=2)\n",
        "\n",
        "            process_ = ((iter_ / iter_process) * 100) % 10\n",
        "            if process_ == 0:\n",
        "                print(\"Process prediction: \" + str(int((iter_ / iter_process) * 100)) + \"%\")\n",
        "        print(\"Process prediction: 100%\")\n",
        "        avg_score, rmse_ls_all, std_ls_all = [], [], []\n",
        "        l_pred_new_well = [[], [], []]\n",
        "        for well_ in range(self.n_well):\n",
        "            l_i = target_split[well_]\n",
        "            l_pred_ls_i = l_pred_ls_tot_four[well_]\n",
        "            avg_score_well, rmse_well, std_well = [], [], []\n",
        "            if new_well:\n",
        "                for var_ in range(n_var):\n",
        "                    y_pred_ = l_pred_ls_i[var_][2:]\n",
        "                    x_ = np.arange(np.shape(y_pred_)[0])\n",
        "                    if self.pred_round:\n",
        "                        w_len = 10\n",
        "                        x_ = x_[(w_len - 2):-1]\n",
        "                        y_pred_ = np.average(np.lib.stride_tricks.sliding_window_view(np.array(y_pred_), window_shape=w_len), axis=1)\n",
        "                    l_pred_new_well[var_].append(y_pred_)\n",
        "                    # if plot_:\n",
        "                    #    title_ = 'Forecast from well no ' + str(well_ + 1) + ' for ' + str(self.var_col[var_])\n",
        "                    #    self.get_plot(x_=x_, y_=y_pred_, y_pred=y_pred_, y_label=title_)\n",
        "            else:\n",
        "                for var_ in range(n_var):\n",
        "                    x_ = time_split[well_][(3 + self.n_old_obs):(3 + self.n_old_obs + n_iter)]\n",
        "                    y_ = l_i[var_][(3 + self.n_old_obs):(3 + self.n_old_obs + n_iter)]\n",
        "                    y_pred_ = l_pred_ls_i[var_][2:]\n",
        "                    if self.pred_round:\n",
        "                        w_len = 10\n",
        "                        x_ = x_[(w_len - 2):-1]\n",
        "                        y_ = y_[(w_len - 2):-1]\n",
        "                        y_pred_ = np.average(np.lib.stride_tricks.sliding_window_view(np.array(y_pred_), window_shape=w_len), axis=1)\n",
        "                    title_ = 'Well no ' + str(well_ + 1) + ' for ' + str(self.var_col[var_])\n",
        "                    if plot_:\n",
        "                        self.get_plot(x_=x_, y_=y_, y_pred=y_pred_, y_label=title_)\n",
        "                    avg_score_well_var, rmse_well_var, std_well_var = self.model_score(y_=y_, y_pred_=y_pred_)\n",
        "                    avg_score_well.append(avg_score_well_var)\n",
        "                    rmse_well.append(rmse_well_var)\n",
        "                    std_well.append(std_well_var)\n",
        "                    if plot_:\n",
        "                        print(\"Average error for the well no \" + str(well_) + \" and \"\n",
        "                              + str(self.var_col[var_]) + \": \" + str(\"{:.4f}\".format(avg_score_well_var)) + \"%\")\n",
        "                        print(\"Model for well number \" + str(well_ + 1) + \" and \" + str(self.var_col[var_]) +\n",
        "                              \" is fitted with RMSE: \" + str(\"{:.4f}\".format(rmse_well_var)) + \".\")\n",
        "                avg_score.append(avg_score_well)\n",
        "                rmse_ls_all.append(rmse_well)\n",
        "                std_ls_all.append(std_well)\n",
        "                if plot_:\n",
        "                    avg_score_well_ = np.average(np.array(avg_score_well))\n",
        "                    print(\"Average error for the well no \" + str(well_) +\n",
        "                          \" for all variables: \" + str(\"{:.4f}\".format(avg_score_well_)) + \"%\")\n",
        "        if not new_well:\n",
        "            avg_score_tot = [np.sum(np.array(arr_)) for arr_ in avg_score]\n",
        "            avg_score_t = np.sum(avg_score_tot) / (self.n_well + n_var)\n",
        "            print(\"Average error for the prediction for all variables and wells: \"\n",
        "                  + str(\"{:.4f}\".format(avg_score_t)) + \"%\")\n",
        "        else:\n",
        "            l_pred_new_well_avg = []\n",
        "            for var_ in range(n_var):\n",
        "                avg_y_pred = np.average(np.array(l_pred_new_well[var_]), axis=0, weights=plot_w)\n",
        "                l_pred_new_well_avg.append(avg_y_pred)\n",
        "                if plot_:\n",
        "                    x_ = np.arange(np.shape(avg_y_pred)[0])\n",
        "                    title_ = 'Average forecast from all wells for ' + str(self.var_col[var_])\n",
        "                    self.get_plot(x_=x_, y_=avg_y_pred, y_pred=avg_y_pred, y_label=title_)\n",
        "\n",
        "        if new_well:\n",
        "            time_ax = np.arange(np.shape(avg_y_pred)[0])\n",
        "            return l_pred_new_well_avg, l_pred_ls_tot_four, time_ax\n",
        "        else:\n",
        "            time_ax = time_split[0][(3 + self.n_old_obs):(3 + self.n_old_obs + n_iter)]\n",
        "            return avg_score_t, target_split, l_pred_ls_tot_four, avg_score, rmse_ls_all, std_ls_all, time_ax\n",
        "\n",
        "    \"\"\" Plot function \"\"\"\n",
        "\n",
        "    def get_plot(self, x_, y_, y_pred, v_l=0, x_label=\"Time (day)\", y_label=\"Oil Rate SC - Daily (mü/day)\"):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x_:\n",
        "            y_:\n",
        "            y_pred_:\n",
        "        Output:\n",
        "            plot\n",
        "        \"\"\"\n",
        "\n",
        "        # plot y_ vs. y_ predicted:\n",
        "        plt.plot(x_, y_)\n",
        "        plt.plot(x_, y_pred)\n",
        "        if v_l != 0:\n",
        "            plt.axvline(x=v_l)\n",
        "        plt.xlabel(x_label)\n",
        "        plt.ylabel(y_label)\n",
        "\n",
        "        # displaying the title\n",
        "        plt.title(y_label)\n",
        "        plt.show()\n",
        "\n",
        "    def model_score(self, y_, y_pred_):\n",
        "        y_arr = np.array(y_)\n",
        "        y_pred_arr = np.array(y_pred_)\n",
        "        avg_score_ = np.average(np.abs(y_arr - y_pred_arr) / y_arr) * 100\n",
        "        rmse_ = np.sqrt(np.sum((y_pred_arr - y_arr) ** 2) / len(y_arr))\n",
        "        std_ = np.std(y_pred_arr - y_arr)\n",
        "        return avg_score_, rmse_, std_\n",
        "\n",
        "    def lin_model(self):\n",
        "        self.model_ = LinearRegression()\n",
        "\n",
        "    def lstm_model(self):\n",
        "        lstm_ = Sequential(\n",
        "            [LSTM(256, return_sequences=True, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "             LSTM(256, return_sequences=True, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "             LSTM(256, return_sequences=True, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "             LSTM(256, return_sequences=True, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "             tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                   activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "             tf.keras.layers.Dense(units=1, kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                   activity_regularizer=tf.keras.regularizers.L2(0.00001))\n",
        "             ])\n",
        "        lstm_.add(Dropout(0.001))\n",
        "        self.model_ = lstm_\n",
        "\n",
        "    def dense_model(self):\n",
        "        dense_ = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "            tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "            tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "            tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "            tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                  activity_regularizer=tf.keras.regularizers.L2(0.00001)),\n",
        "            tf.keras.layers.Dense(units=1, kernel_regularizer=tf.keras.regularizers.L1(0.000001),\n",
        "                                  activity_regularizer=tf.keras.regularizers.L2(0.00001))\n",
        "        ])\n",
        "        self.model_ = dense_\n",
        "\n",
        "    def make_dataset_ts(self, data_train, data_target):\n",
        "        data_train_ls, data_train_rs_ls, data_target_ls, ds_ls = [], [], [], []\n",
        "        for var_ in range(len(self.var_col)):\n",
        "            data_target_var = data_target[var_]\n",
        "            data_target_var = np.array(data_target_var, dtype=np.float32)\n",
        "            data_target_ls.append(data_target_var)\n",
        "        data_train = np.array(data_train, dtype=np.float32)\n",
        "        data_train_rs = data_train.reshape((data_train.shape[0], 1, data_train.shape[1]))\n",
        "        return data_train, data_train_rs, data_target_ls\n",
        "\n",
        "    def compile_and_fit(self, model, data):\n",
        "        model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                      optimizer=tf.optimizers.Adam(),\n",
        "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "        history = model.fit(data[0], data[1], epochs=self.epochs, validation_data=data)\n",
        "        return history\n",
        "\n",
        "    @tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
        "    def predict_tf(self, model_, data_):\n",
        "        result_ = model_.predict(data_)\n",
        "        return result_[:, 0, 0]\n",
        "\n",
        "    def get_new_well_init(self, df_init_ls, df_new_ls, new_factor):\n",
        "\n",
        "        # new well position:\n",
        "        new_well_position = self.get_well_position(df_list=df_new_ls, init_=False)\n",
        "\n",
        "        # well positions matrix function:\n",
        "        init_well_position = self.get_well_position(df_list=df_init_ls)\n",
        "\n",
        "        # add new well positions to the matrix:\n",
        "        well_position = np.append(init_well_position, [new_well_position], axis=0)\n",
        "\n",
        "        # weighting distance matrix function:\n",
        "        weight_matrix = self.get_weight(well_position=well_position)\n",
        "\n",
        "        # get the weight from the other wells:\n",
        "        weight_arr_update = np.atleast_2d(weight_matrix[-1, :-1])\n",
        "\n",
        "        weight_matrix_update = []\n",
        "        for well_ in range(self.n_well):\n",
        "            weight_matrix_update.append(weight_arr_update)\n",
        "        weight_matrix_update = np.array(weight_matrix_update)\n",
        "\n",
        "        old_factor = self.factor_\n",
        "        self.factor_ = new_factor\n",
        "        weight_matrix_fin = self.get_weight(well_position=well_position)\n",
        "        weight_arr_update_fin = np.atleast_2d(weight_matrix_fin[-1, :-1])\n",
        "        final_weight = weight_arr_update_fin[0, :]\n",
        "        self.factor_ = old_factor\n",
        "\n",
        "        return weight_matrix_update, final_weight\n",
        "\n",
        "    def pipeline_new_well(self, df_init_ls, df_new_ls, model_ls_tr, n_iter=100, new_factor=2, plot_=False):\n",
        "\n",
        "        self.train_ = (10 / 15 * 0.4)\n",
        "        self.valid_ = (10 / 15 * 0.4)\n",
        "        self.test_ = (10 / 15 * 0.4)\n",
        "\n",
        "        w_mat, f_weight = self.get_new_well_init(df_init_ls=df_init_ls, df_new_ls=df_new_ls, new_factor=new_factor)\n",
        "        var_ls_train, var_ls_test, const_ls_train, const_ls_test, weight_matrix, \\\n",
        "        time_train, time_test, grad_w_train_ls, grad_w_test_ls, coor_train_ls, \\\n",
        "        coor_test_ls, var_ls_val, const_ls_val, time_val, grad_w_val_ls, coor_val_ls = \\\n",
        "            self.pipeline_init(df_list=df_init_ls, init_=True, weight_matrix=w_mat)\n",
        "\n",
        "        var_, const_, time_, grad_, coor_, target_ = \\\n",
        "            ts_model.split_data(var_w_ls_=var_ls_test, const_w_ls_=const_ls_test,\n",
        "                                time_=time_test, grad_w_ls_=grad_w_test_ls, coor_ls_=coor_test_ls,\n",
        "                                title_=\"endings\")\n",
        "\n",
        "        var_en_ls, const_en_ls, time_en_ls, grad_en_ls, coor_en_ls, target_en_ls = \\\n",
        "            [], [], [], [], [], []\n",
        "\n",
        "        for well_ in range(self.n_well):\n",
        "            var_en, const_en, time_en, grad_en, coor_en, target_en = \\\n",
        "                ts_model.get_endings(var_=var_[well_], const_=const_[well_], time_=time_[well_],\n",
        "                                     grad_=grad_[well_], coor_=coor_[well_], target_=target_[well_])\n",
        "            var_en_ls.append(var_en)\n",
        "            const_en_ls.append(const_en)\n",
        "            time_en_ls.append(time_en)\n",
        "            grad_en_ls.append(grad_en)\n",
        "            coor_en_ls.append(coor_en)\n",
        "            target_en_ls.append(target_en)\n",
        "\n",
        "        for well_ in range(self.n_well):\n",
        "            for col_ in range(len(self.const_col)):\n",
        "                val_ = df_new_ls[self.const_col[col_]].values\n",
        "                const_en_ls[well_][col_][:, well_] = val_[0]\n",
        "            for coor_ in range(len(self.col_coor)):\n",
        "                val_ = df_new_ls[self.col_coor[coor_]].values\n",
        "                coor_en_ls[well_][coor_][:] = val_[0]\n",
        "\n",
        "        l_pred_, l_pred_ls, time_ax = self.get_forecast_split(model_=model_ls_tr, n_iter=n_iter,\n",
        "                                                              weight_matrix=weight_matrix,\n",
        "                                                              var_split=var_en_ls, const_split=const_en_ls,\n",
        "                                                              time_split=time_en_ls, grad_split=grad_en_ls,\n",
        "                                                              coor_split=coor_en_ls, target_split=target_en_ls,\n",
        "                                                              plot_=plot_, new_well=True, plot_w=f_weight)\n",
        "        return l_pred_, l_pred_ls, time_ax\n",
        "\n",
        "    \"\"\" New well generation function based on the --over data-- method \"\"\"\n",
        "\n",
        "    def pipeline_new_well_over_data(self, df_init_ls, df_new_ls, model_ls_tr, n_iter=1825, new_factor=3, plot_=False):\n",
        "\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            df_init_ls: a list of dataframes for known wells (n_df, df)\n",
        "            df_new_ls: a list of dataframes for a new well (to extract the constants and location)\n",
        "\t\t\tn_iter: the number of days from the end of the dataset\n",
        "\t\t\tmodel_ls_tr: the list of model based on well number\n",
        "\t\t\tnew_factor: factor for inverse distance weighting in averaging the value \n",
        "\t\t\tplot_: optional argument, if True then plots will appear during the computation\n",
        "        Output:\n",
        "            l_pred_ls_w: a list of generated values for a new well (n_var)\n",
        "            time_en_ls: a time line for the list above\n",
        "        \"\"\"\n",
        "\n",
        "        # New weighting matrix\n",
        "        w_mat, f_weight = self.get_new_well_init(df_init_ls=df_init_ls, df_new_ls=df_new_ls, new_factor=new_factor)\n",
        "\n",
        "        # Change in the time line to extract the latest 5 years\n",
        "        self.train_ = 0.01\n",
        "        self.valid_ = 0.01\n",
        "        self.test_ = 0.98\n",
        "\n",
        "        # Get the pre-processed data from known wells\n",
        "        var_ls_train, var_ls_test, const_ls_train, const_ls_test, weight_matrix, \\\n",
        "        time_train, time_test, grad_w_train_ls, grad_w_test_ls, coor_train_ls, \\\n",
        "        coor_test_ls, var_ls_val, const_ls_val, time_val, grad_w_val_ls, coor_val_ls = \\\n",
        "            self.pipeline_init(df_list=df_init_ls, init_=True, weight_matrix=w_mat)\n",
        "\n",
        "        # Split the data\n",
        "        var_, const_, time_, grad_, coor_, target_ = \\\n",
        "            ts_model.split_data(var_w_ls_=var_ls_test, const_w_ls_=const_ls_test,\n",
        "                                time_=time_test, grad_w_ls_=grad_w_test_ls, coor_ls_=coor_test_ls,\n",
        "                                title_=\"endings\")\n",
        "\n",
        "        # Initial the new lists for\n",
        "        var_en_ls, const_en_ls, time_en_ls, grad_en_ls, coor_en_ls, target_en_ls = \\\n",
        "            [], [], [], [], [], []\n",
        "\n",
        "        # Get the last observations for the known wells (5 years)\n",
        "        for well_ in range(self.n_well):\n",
        "            var_en, const_en, time_en, grad_en, coor_en, target_en = \\\n",
        "                ts_model.get_endings(var_=var_[well_], const_=const_[well_], time_=time_[well_],\n",
        "                                     grad_=grad_[well_], coor_=coor_[well_], target_=target_[well_],\n",
        "                                     minus_start=n_iter)\n",
        "            var_en_ls.append(var_en)\n",
        "            const_en_ls.append(const_en)\n",
        "            time_en_ls.append(time_en)\n",
        "            grad_en_ls.append(grad_en)\n",
        "            coor_en_ls.append(coor_en)\n",
        "            target_en_ls.append(target_en)\n",
        "\n",
        "        # Change the values in the constants and coordinates for the new well parameters\n",
        "        for well_ in range(self.n_well):\n",
        "            for col_ in range(len(self.const_col)):\n",
        "                val_ = df_new_ls[self.const_col[col_]].values\n",
        "                const_en_ls[well_][col_][:, well_] = val_[0]\n",
        "            for coor_ in range(len(self.col_coor)):\n",
        "                val_ = df_new_ls[self.col_coor[coor_]].values\n",
        "                coor_en_ls[well_][coor_][:] = val_[0]\n",
        "\n",
        "            # Make the prediction with --over data-- method\n",
        "        l_ls, l_pred_ls, avg_score, rmse_ls_all, std_ls_all, time_ax = self.get_prediction(model_=model_ls_tr,\n",
        "                                                                                           var_split=var_en_ls,\n",
        "                                                                                           const_split=const_en_ls,\n",
        "                                                                                           time_split=time_en_ls,\n",
        "                                                                                           grad_split=grad_en_ls,\n",
        "                                                                                           coor_split=coor_en_ls,\n",
        "                                                                                           target_split=target_en_ls,\n",
        "                                                                                           plot_=False,\n",
        "                                                                                           new_well=True)\n",
        "\n",
        "        # Average the results from the models by inverse distance weights\n",
        "        l_pred_ls_w = np.average(np.array(l_pred_ls), axis=0, weights=f_weight)\n",
        "\n",
        "        # Optional: if True - plot the results\n",
        "        if plot_:\n",
        "            for var_ in range(len(self.var_col)):\n",
        "                x_ = time_en_ls[0][self.n_old_obs:]\n",
        "                title_ = 'Forecast for a new well based on the dataset for ' + str(self.var_col[var_])\n",
        "                self.get_plot(x_=x_, y_=l_pred_ls_w[var_], y_pred=l_pred_ls_w[var_], y_label=title_)\n",
        "\n",
        "            # REturn the results and the timeline\n",
        "        return l_pred_ls_w, time_en_ls[0][self.n_old_obs:]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLtI-rQra4mU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Prepare the directory."
      ],
      "metadata": {
        "id": "lNSPKRrIQ0pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################################################\n",
        "\n",
        "##############################################################\n",
        "###############  CALL THE TimeSeriesModel  ###################\n",
        "\n",
        "##############################################################\n",
        "################  PREPARE THE DIRECTORY  #####################\n",
        "\n",
        "# Directory folders for the output.\n",
        "out_f = \"./Output_new\"\n",
        "mod_t = \"./model_temp\"\n",
        "dir_linear = \"/AR\"\n",
        "dir_dense = \"/Dense\"\n",
        "dir_lstm = \"/LSTM\"\n",
        "models_ = [dir_linear, dir_dense, dir_lstm]\n",
        "fit_f = \"/Fit\"\n",
        "new_well_over_data = \"/New_well_over_data\"\n",
        "new_well_forward = \"/New_well_walk_forward\"\n",
        "test_over_data = \"/Test_over_data\"\n",
        "test_walk_forward = \"/Test_walk_forward\"\n",
        "out_folders = [fit_f, new_well_over_data, new_well_forward, test_over_data, test_walk_forward]\n",
        "\n",
        "if not os.path.exists(mod_t):\n",
        "    os.mkdir(mod_t)\n",
        "if not os.path.exists(out_f):\n",
        "    os.mkdir(out_f)\n",
        "    for i in range(len(models_)):\n",
        "        os.mkdir(out_f + models_[i])\n",
        "        for j in range(len(out_folders)):\n",
        "            os.mkdir(out_f + models_[i] + out_folders[j])\n",
        "\n",
        "# Directory folders for the output.\n",
        "fit_f = \"Fit/\"\n",
        "new_well_over_data = \"New_well_over_data/\"\n",
        "new_well_forward = \"New_well_walk_forward/\"\n",
        "test_over_data = \"Test_over_data/\"\n",
        "test_walk_forward = \"Test_walk_forward/\"\n",
        "\n",
        "# Time line for training, validation and testing\n",
        "train_time = (10 / 15 * 0.4)\n",
        "valid_time = (10 / 15 * 0.2)\n",
        "test_time = (10 / 15 * 0.4)\n"
      ],
      "metadata": {
        "id": "fyzsWd54dH_G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Choose the model."
      ],
      "metadata": {
        "id": "8DAH2xI0RCUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "######################  AUTOREGRESSION  ######################\n",
        "'''\n",
        "# Directory folder\n",
        "dir_ts = \"./Output_new/AR/\"\n",
        "\n",
        "# Import data\n",
        "ts_model = TimeSeriesModel(n_old_obs=25, lin_=True, dense_=False, lstm_=False, train_=train_time,\n",
        "                           valid_=valid_time, test_=test_time, n_well=8, factor_=20, mod_update=100,\n",
        "                           w_model_init=0.5, w_model_update=0.5)\n",
        "\n",
        "##############################################################\n",
        "########################## DENSE  ############################\n",
        "'''\n",
        "# Directory folder\n",
        "dir_ts = \"./Output_new/Dense/\"\n",
        "\n",
        "# Import data\n",
        "ts_model = TimeSeriesModel(n_old_obs=5, lin_=False, dense_=True, lstm_=False, train_=train_time,\n",
        "                           valid_=valid_time, test_=test_time, n_well=8, factor_=20, mod_update=1000000,\n",
        "                           w_model_init=0.5, w_model_update=0.5, epochs=50)\n",
        "'''\n",
        "##############################################################\n",
        "########################### LSTM  ############################\n",
        "\n",
        "# Directory folder\n",
        "dir_ts = \"./Output_new/LSTM/\"\n",
        "\n",
        "# Import data\n",
        "ts_model = TimeSeriesModel(n_old_obs=5, lin_=False, dense_=False, lstm_=True, train_=train_time,\n",
        "                           valid_=valid_time, test_=test_time, n_well=8, factor_=20, mod_update=1000000,\n",
        "                           w_model_init=0.5, w_model_update=0.5, epochs=50)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PH2gudKDRBU6",
        "outputId": "d50161e6-199b-4858-c141-b992f5367e33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n##############################################################\\n########################### LSTM  ############################\\n\\n# Directory folder\\ndir_ts = \"./Output_new/LSTM/\"\\n\\n# Import data\\nts_model = TimeSeriesModel(n_old_obs=5, lin_=False, dense_=False, lstm_=True, train_=train_time,\\n                           valid_=valid_time, test_=test_time, n_well=8, factor_=20, mod_update=1000000,\\n                           w_model_init=0.5, w_model_update=0.5, epochs=50)\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Pre-process the dataset."
      ],
      "metadata": {
        "id": "okYZjvMyRiTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##############################################################\n",
        "######################### SCRIPT  ############################\n",
        "\n",
        "dir_wells = ts_model.dir_\n",
        "pro_df_r_ls, pro_df_ls, norm_ = ts_model.get_df_list(dir_=dir_wells, norm_=None)\n",
        "\n",
        "# Parameters\n",
        "var_col = ts_model.var_col\n",
        "col_time = ts_model.col_time\n",
        "col_coor = ts_model.col_coor\n",
        "const_col = ts_model.const_col\n",
        "n_old_obs = ts_model.n_old_obs\n",
        "n_well = ts_model.n_well\n",
        "n_var = len(var_col)\n",
        "train_ = ts_model.train_\n",
        "n_rows = len(pro_df_ls[0])\n",
        "last_tr_row = int(n_rows * train_) - (n_old_obs - 1)\n",
        "\n",
        "# Preprocess the data\n",
        "var_ls_train, var_ls_test, const_ls_train, const_ls_test, weight_matrix, \\\n",
        "time_train, time_test, grad_w_train_ls, grad_w_test_ls, coor_train_ls, \\\n",
        "coor_test_ls, var_ls_val, const_ls_val, time_val, grad_w_val_ls, coor_val_ls = \\\n",
        "    ts_model.pipeline_init(df_list=pro_df_ls, init_=True, weight_matrix=None)\n",
        "\n",
        "var_split_tr, const_split_tr, time_split_tr, grad_split_tr, coor_split_tr, target_split_tr = \\\n",
        "    ts_model.split_data(var_w_ls_=var_ls_train, const_w_ls_=const_ls_train,\n",
        "                        time_=time_train, grad_w_ls_=grad_w_train_ls, coor_ls_=coor_train_ls,\n",
        "                        title_=\"training\")\n",
        "\n",
        "var_split_vl, const_split_vl, time_split_vl, grad_split_vl, coor_split_vl, target_split_vl = \\\n",
        "    ts_model.split_data(var_w_ls_=var_ls_val, const_w_ls_=const_ls_val,\n",
        "                        time_=time_val, grad_w_ls_=grad_w_val_ls, coor_ls_=coor_val_ls,\n",
        "                        title_=\"validation\")\n",
        "\n",
        "var_split_ts, const_split_ts, time_split_ts, grad_split_ts, coor_split_ts, target_split_ts = \\\n",
        "    ts_model.split_data(var_w_ls_=var_ls_test, const_w_ls_=const_ls_test,\n",
        "                        time_=time_test, grad_w_ls_=grad_w_test_ls, coor_ls_=coor_test_ls,\n",
        "                        title_=\"testing\")\n",
        "\n",
        "'''\n",
        "# !!!!!!!!! Only for AR model !!!!!!!!!\n",
        "# Validate the AR model:\n",
        "n_old_obs_AR, mod_update_AR, w_model_init_AR = ts_model.get_linear_valid_model(weight_matrix=weight_matrix,\n",
        "                                                                               var_train=var_split_tr,\n",
        "                                                                               const_train=const_split_tr,\n",
        "                                                                               time_train=time_split_tr,\n",
        "                                                                               grad_train=grad_split_tr,\n",
        "                                                                               coor_train=coor_split_tr,\n",
        "                                                                               target_train=target_split_tr,\n",
        "                                                                               var_val=var_split_vl,\n",
        "                                                                               const_val=const_split_vl,\n",
        "                                                                               time_val=time_split_vl,\n",
        "                                                                               grad_val=grad_split_vl,\n",
        "                                                                               coor_val=coor_split_vl,\n",
        "                                                                               target_val=target_split_vl)\n",
        "# Store the results:\n",
        "dir_fit = dir_ts\n",
        "f_dir = dir_fit + (\"AR_validate_param.csv\")\n",
        "df_param = pd.DataFrame([n_old_obs_AR, mod_update_AR, w_model_init_AR])\n",
        "df_param.to_csv(f_dir)\n",
        "'''\n",
        "\n",
        "# After validation:\n",
        "var_split_tr, const_split_tr, time_split_tr, grad_split_tr, coor_split_tr, target_split_tr = \\\n",
        "    ts_model.connect_data(var_o=var_split_tr, const_o=const_split_tr, time_o=time_split_tr, grad_o=grad_split_tr,\n",
        "                          coor_o=coor_split_tr, target_o=target_split_tr, var_n=var_split_vl, const_n=const_split_vl,\n",
        "                          time_n=time_split_vl, grad_n=grad_split_vl, coor_n=coor_split_vl, target_n=target_split_vl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVlHAFCmKG3e",
        "outputId": "e3a8f42e-6bdb-499c-9a2b-2e3bc611b234"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data are imported.\n",
            "Date are separated to variables, gradients, constants, coordinates, and time.\n",
            "Date are splitted to each well for training dataset.\n",
            "Date are splitted to each well for validation dataset.\n",
            "Date are splitted to each well for testing dataset.\n",
            "Provided data are connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Test if the dataset has a proper format and values."
      ],
      "metadata": {
        "id": "4OHeukSHRqKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sum of the data from variable array:\")\n",
        "print(np.sum(var_split_tr[0]))\n",
        "print(\"Sum of the data from constant array:\")\n",
        "print(np.sum(const_split_tr[0]))\n",
        "print(\"Sum of the data from time array:\")\n",
        "print(np.sum(time_split_tr[0]))\n",
        "print(\"Sum of the data from gradient array:\")\n",
        "print(np.sum(grad_split_tr[0]))\n",
        "print(\"Sum of the data from coordinates array:\")\n",
        "print(np.sum(coor_split_tr[0]))\n",
        "print(\"Sum of the data from target array:\")\n",
        "print(np.sum(target_split_tr[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Mu6dfCnaYZ",
        "outputId": "e643e0ab-2949-4255-e79d-2d976347ed93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of the data from variable array:\n",
            "334363715.79172957\n",
            "Sum of the data from constant array:\n",
            "-4642.135995390275\n",
            "Sum of the data from time array:\n",
            "2401336\n",
            "Sum of the data from gradient array:\n",
            "223217.35315173774\n",
            "Sum of the data from coordinates array:\n",
            "54775\n",
            "Sum of the data from target array:\n",
            "334363715.79172677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output (note: results will be slightly different in decimal places): \n",
        "\n",
        "Sum of the data from variable array:\n",
        "334363715.7917299\n",
        "\n",
        "Sum of the data from constant array:\n",
        "-4642.135995390275\n",
        "\n",
        "Sum of the data from time array:\n",
        "2401336\n",
        "\n",
        "Sum of the data from gradient array:\n",
        "223217.35315173812\n",
        "\n",
        "Sum of the data from coordinates array:\n",
        "54775\n",
        "\n",
        "Sum of the data from target array:\n",
        "334363715.7917272"
      ],
      "metadata": {
        "id": "FelYoD0OoIuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Fit the model ( it takes a while to execute )."
      ],
      "metadata": {
        "id": "eBG5Yho-R2xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model validation:\n",
        "a_ls_tr, l_ls, l_pred_ls, model_ls_tr, avg_score, rmse_ls_all, std_ls_all, time_ax = \\\n",
        "    ts_model.pipeline_model_split(var_split=var_split_tr, const_split=const_split_tr,\n",
        "                                  time_split=time_split_tr, grad_split=grad_split_tr,\n",
        "                                  coor_split=coor_split_tr, target_split=target_split_tr, plot_=True)\n",
        "\n",
        "# Store the results:\n",
        "dir_fit = dir_ts + fit_f\n",
        "for well_ in range(n_well):\n",
        "    for var_ in range(n_var):\n",
        "        f_dir = dir_fit + \"Well_\" + str(well_ + 1) + \"_\" + str(var_col[var_])\n",
        "        df_l = pd.DataFrame(l_ls[well_][var_])\n",
        "        df_l.to_csv(f_dir + \"_train.csv\")\n",
        "        df_e = pd.DataFrame([avg_score[well_][var_]])\n",
        "        df_e.to_csv(f_dir + \"_error.csv\")\n",
        "        df_rmse = pd.DataFrame([rmse_ls_all[well_][var_]])\n",
        "        df_rmse.to_csv(f_dir + \"_rmse.csv\")\n",
        "        df_std = pd.DataFrame([std_ls_all[well_][var_]])\n",
        "        df_std.to_csv(f_dir + \"_std.csv\")\n",
        "        df_time = pd.DataFrame(time_ax)\n",
        "        df_time.to_csv(f_dir + \"_time.csv\")\n",
        "    f_dir = dir_fit + \"Well_\" + str(well_ + 1) + \"_Prediction\"\n",
        "    df_l_pred = pd.DataFrame(l_pred_ls[well_])\n",
        "    df_l_pred.to_csv(f_dir + \"_pred.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k2SSTKbJR1IE",
        "outputId": "ac3e095a-0779-4316-b63f-b5ff6999156c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset for well no. 1 is prepared for keras.\n",
            "Epoch 1/50\n",
            "69/69 [==============================] - 2s 27ms/step - loss: 4757726.5000 - mean_absolute_error: 707.1121 - val_loss: 694313.8125 - val_mean_absolute_error: 47.2696\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 560414.2500 - mean_absolute_error: 46.5113 - val_loss: 465962.2500 - val_mean_absolute_error: 28.7936\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - 1s 13ms/step - loss: 426569.6875 - mean_absolute_error: 22.4907 - val_loss: 394377.7812 - val_mean_absolute_error: 18.9467\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 371154.7812 - mean_absolute_error: 19.3736 - val_loss: 348004.2500 - val_mean_absolute_error: 12.2220\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 328134.3438 - mean_absolute_error: 15.6679 - val_loss: 307805.7500 - val_mean_absolute_error: 17.4275\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 1s 13ms/step - loss: 289520.8125 - mean_absolute_error: 13.7051 - val_loss: 270901.0938 - val_mean_absolute_error: 15.5717\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - 1s 13ms/step - loss: 254226.7344 - mean_absolute_error: 14.2327 - val_loss: 237120.2344 - val_mean_absolute_error: 13.5242\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 221855.1406 - mean_absolute_error: 11.4013 - val_loss: 206214.1875 - val_mean_absolute_error: 12.9564\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 192491.8594 - mean_absolute_error: 10.6293 - val_loss: 178806.1250 - val_mean_absolute_error: 12.8754\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 166524.2656 - mean_absolute_error: 16.4147 - val_loss: 153684.4062 - val_mean_absolute_error: 15.1785\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 142424.5469 - mean_absolute_error: 11.7642 - val_loss: 131274.9688 - val_mean_absolute_error: 11.1422\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 121445.7734 - mean_absolute_error: 9.3302 - val_loss: 111612.3750 - val_mean_absolute_error: 9.0926\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 103022.9766 - mean_absolute_error: 9.7225 - val_loss: 94596.5703 - val_mean_absolute_error: 14.7227\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 86915.5078 - mean_absolute_error: 8.3286 - val_loss: 79452.4531 - val_mean_absolute_error: 6.7788\n",
            "Epoch 15/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 73049.5156 - mean_absolute_error: 9.1900 - val_loss: 66468.6406 - val_mean_absolute_error: 6.8084\n",
            "Epoch 16/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 61069.9414 - mean_absolute_error: 8.2453 - val_loss: 55521.7148 - val_mean_absolute_error: 5.4359\n",
            "Epoch 17/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 50868.5391 - mean_absolute_error: 8.5789 - val_loss: 46172.9844 - val_mean_absolute_error: 8.0886\n",
            "Epoch 18/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 45060.6875 - mean_absolute_error: 24.0038 - val_loss: 49930.5859 - val_mean_absolute_error: 86.1412\n",
            "Epoch 19/50\n",
            "69/69 [==============================] - 0s 7ms/step - loss: 36617.2031 - mean_absolute_error: 17.1048 - val_loss: 32221.1348 - val_mean_absolute_error: 13.6528\n",
            "Epoch 20/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 28994.2188 - mean_absolute_error: 8.0787 - val_loss: 25939.8125 - val_mean_absolute_error: 7.3194\n",
            "Epoch 21/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 23425.9883 - mean_absolute_error: 7.4962 - val_loss: 20949.0703 - val_mean_absolute_error: 7.5151\n",
            "Epoch 22/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 18857.9336 - mean_absolute_error: 6.8329 - val_loss: 16808.2461 - val_mean_absolute_error: 5.5359\n",
            "Epoch 23/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 15097.0537 - mean_absolute_error: 5.8425 - val_loss: 13409.8105 - val_mean_absolute_error: 4.9727\n",
            "Epoch 24/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 12035.7920 - mean_absolute_error: 5.2158 - val_loss: 10651.6279 - val_mean_absolute_error: 4.4794\n",
            "Epoch 25/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 9565.2236 - mean_absolute_error: 5.8053 - val_loss: 8470.9307 - val_mean_absolute_error: 6.1773\n",
            "Epoch 26/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 7534.8726 - mean_absolute_error: 4.8275 - val_loss: 6624.4585 - val_mean_absolute_error: 3.9684\n",
            "Epoch 27/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 5896.4321 - mean_absolute_error: 3.9325 - val_loss: 5235.1597 - val_mean_absolute_error: 7.1220\n",
            "Epoch 28/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 4645.2666 - mean_absolute_error: 6.3286 - val_loss: 4041.8962 - val_mean_absolute_error: 5.4860\n",
            "Epoch 29/50\n",
            "69/69 [==============================] - 0s 7ms/step - loss: 3570.1111 - mean_absolute_error: 4.7222 - val_loss: 3110.3813 - val_mean_absolute_error: 4.2645\n",
            "Epoch 30/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2775.6753 - mean_absolute_error: 6.1180 - val_loss: 2413.8843 - val_mean_absolute_error: 4.3823\n",
            "Epoch 31/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2127.5649 - mean_absolute_error: 6.1230 - val_loss: 1821.8466 - val_mean_absolute_error: 4.3139\n",
            "Epoch 32/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1607.9628 - mean_absolute_error: 5.0715 - val_loss: 1380.1476 - val_mean_absolute_error: 4.3751\n",
            "Epoch 33/50\n",
            "69/69 [==============================] - 0s 7ms/step - loss: 1219.6901 - mean_absolute_error: 4.8289 - val_loss: 1047.7596 - val_mean_absolute_error: 5.7207\n",
            "Epoch 34/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 929.4789 - mean_absolute_error: 5.4782 - val_loss: 775.2875 - val_mean_absolute_error: 3.5756\n",
            "Epoch 35/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 987.2029 - mean_absolute_error: 11.4835 - val_loss: 1261.6714 - val_mean_absolute_error: 20.0904\n",
            "Epoch 36/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 673.1484 - mean_absolute_error: 7.4799 - val_loss: 498.3592 - val_mean_absolute_error: 4.6609\n",
            "Epoch 37/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 446.7422 - mean_absolute_error: 6.0783 - val_loss: 430.5150 - val_mean_absolute_error: 8.9350\n",
            "Epoch 38/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 311.3243 - mean_absolute_error: 4.9510 - val_loss: 248.5192 - val_mean_absolute_error: 3.7941\n",
            "Epoch 39/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 231.8323 - mean_absolute_error: 4.8973 - val_loss: 217.8087 - val_mean_absolute_error: 6.4949\n",
            "Epoch 40/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 215.9352 - mean_absolute_error: 6.9979 - val_loss: 195.9822 - val_mean_absolute_error: 7.3600\n",
            "Epoch 41/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 150.0641 - mean_absolute_error: 5.5378 - val_loss: 158.8111 - val_mean_absolute_error: 6.6713\n",
            "Epoch 42/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 120.1182 - mean_absolute_error: 4.9914 - val_loss: 89.9252 - val_mean_absolute_error: 3.2023\n",
            "Epoch 43/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 82.0412 - mean_absolute_error: 3.6365 - val_loss: 84.9519 - val_mean_absolute_error: 4.7316\n",
            "Epoch 44/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 90.5225 - mean_absolute_error: 4.9983 - val_loss: 89.9384 - val_mean_absolute_error: 5.4571\n",
            "Epoch 45/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 67.5354 - mean_absolute_error: 4.3142 - val_loss: 52.7494 - val_mean_absolute_error: 3.2830\n",
            "Epoch 46/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 58.7970 - mean_absolute_error: 3.9866 - val_loss: 48.2642 - val_mean_absolute_error: 3.5435\n",
            "Epoch 47/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 737.4525 - mean_absolute_error: 17.1852 - val_loss: 491.1706 - val_mean_absolute_error: 15.7904\n",
            "Epoch 48/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 212.1943 - mean_absolute_error: 8.9421 - val_loss: 136.1577 - val_mean_absolute_error: 7.2127\n",
            "Epoch 49/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 90.5169 - mean_absolute_error: 4.7889 - val_loss: 174.5383 - val_mean_absolute_error: 9.1430\n",
            "Epoch 50/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 79.1808 - mean_absolute_error: 4.9617 - val_loss: 92.8289 - val_mean_absolute_error: 6.1420\n",
            "Epoch 1/50\n",
            "69/69 [==============================] - 1s 12ms/step - loss: 4151057152.0000 - mean_absolute_error: 31269.4531 - val_loss: 1825005.6250 - val_mean_absolute_error: 680.2492\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1388843.6250 - mean_absolute_error: 253.6882 - val_loss: 1290429.7500 - val_mean_absolute_error: 131.2402\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - 1s 11ms/step - loss: 1390110.6250 - mean_absolute_error: 256.1868 - val_loss: 1268044.5000 - val_mean_absolute_error: 81.8321\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 1284195.1250 - mean_absolute_error: 131.8313 - val_loss: 1270663.3750 - val_mean_absolute_error: 116.8646\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 1258646.6250 - mean_absolute_error: 79.5461 - val_loss: 1255043.0000 - val_mean_absolute_error: 68.2266\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 0s 7ms/step - loss: 1255807.0000 - mean_absolute_error: 77.9425 - val_loss: 1255320.5000 - val_mean_absolute_error: 82.5147\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1254233.7500 - mean_absolute_error: 81.5424 - val_loss: 1255663.5000 - val_mean_absolute_error: 91.0943\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1255481.1250 - mean_absolute_error: 93.1594 - val_loss: 1254001.8750 - val_mean_absolute_error: 93.5759\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1247301.3750 - mean_absolute_error: 70.8423 - val_loss: 1245273.2500 - val_mean_absolute_error: 81.9632\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1249502.5000 - mean_absolute_error: 95.9689 - val_loss: 1237142.8750 - val_mean_absolute_error: 50.4545\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1248755.5000 - mean_absolute_error: 88.7719 - val_loss: 1457972.8750 - val_mean_absolute_error: 416.6206\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1316504.6250 - mean_absolute_error: 163.3045 - val_loss: 1229685.2500 - val_mean_absolute_error: 44.3819\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1230067.8750 - mean_absolute_error: 61.9479 - val_loss: 1227975.7500 - val_mean_absolute_error: 63.9717\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1231191.2500 - mean_absolute_error: 73.4777 - val_loss: 1454142.5000 - val_mean_absolute_error: 422.9380\n",
            "Epoch 15/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1351030.7500 - mean_absolute_error: 269.1615 - val_loss: 1217204.3750 - val_mean_absolute_error: 38.8561\n",
            "Epoch 16/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1223036.5000 - mean_absolute_error: 80.1882 - val_loss: 1212907.1250 - val_mean_absolute_error: 41.3088\n",
            "Epoch 17/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1213330.6250 - mean_absolute_error: 60.3429 - val_loss: 1206957.1250 - val_mean_absolute_error: 36.3178\n",
            "Epoch 18/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1238724.8750 - mean_absolute_error: 131.8875 - val_loss: 1222069.1250 - val_mean_absolute_error: 136.5045\n",
            "Epoch 19/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1329219.8750 - mean_absolute_error: 268.4420 - val_loss: 1384671.5000 - val_mean_absolute_error: 370.9434\n",
            "Epoch 20/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1219471.1250 - mean_absolute_error: 100.6280 - val_loss: 1194741.2500 - val_mean_absolute_error: 33.2128\n",
            "Epoch 21/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1203456.3750 - mean_absolute_error: 86.5644 - val_loss: 1189454.5000 - val_mean_absolute_error: 29.8020\n",
            "Epoch 22/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1299992.5000 - mean_absolute_error: 220.0011 - val_loss: 10226476.0000 - val_mean_absolute_error: 2548.1021\n",
            "Epoch 23/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1728738.3750 - mean_absolute_error: 409.1233 - val_loss: 1220800.0000 - val_mean_absolute_error: 163.7501\n",
            "Epoch 24/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1233233.5000 - mean_absolute_error: 144.4617 - val_loss: 1191104.0000 - val_mean_absolute_error: 72.6849\n",
            "Epoch 25/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 1186776.1250 - mean_absolute_error: 60.4433 - val_loss: 1180574.8750 - val_mean_absolute_error: 33.3254\n",
            "Epoch 26/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1182253.6250 - mean_absolute_error: 56.7453 - val_loss: 1176527.7500 - val_mean_absolute_error: 36.0942\n",
            "Epoch 27/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1177385.8750 - mean_absolute_error: 55.2493 - val_loss: 1173005.6250 - val_mean_absolute_error: 49.7270\n",
            "Epoch 28/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1176896.1250 - mean_absolute_error: 67.2841 - val_loss: 1168174.8750 - val_mean_absolute_error: 39.3022\n",
            "Epoch 29/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1202946.7500 - mean_absolute_error: 139.3457 - val_loss: 1166359.0000 - val_mean_absolute_error: 56.8803\n",
            "Epoch 30/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2365812.7500 - mean_absolute_error: 731.4307 - val_loss: 1552479.5000 - val_mean_absolute_error: 547.8754\n",
            "Epoch 31/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1347256.2500 - mean_absolute_error: 317.4275 - val_loss: 1304296.6250 - val_mean_absolute_error: 319.5695\n",
            "Epoch 32/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1249875.0000 - mean_absolute_error: 205.2348 - val_loss: 1186195.0000 - val_mean_absolute_error: 138.9893\n",
            "Epoch 33/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1167451.3750 - mean_absolute_error: 75.0146 - val_loss: 1163251.2500 - val_mean_absolute_error: 81.9854\n",
            "Epoch 34/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1167578.8750 - mean_absolute_error: 92.5471 - val_loss: 1164755.0000 - val_mean_absolute_error: 97.3838\n",
            "Epoch 35/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1168427.6250 - mean_absolute_error: 97.5841 - val_loss: 1165025.1250 - val_mean_absolute_error: 114.7996\n",
            "Epoch 36/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1254654.5000 - mean_absolute_error: 195.1916 - val_loss: 2123565.7500 - val_mean_absolute_error: 811.5978\n",
            "Epoch 37/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1802794.5000 - mean_absolute_error: 521.5516 - val_loss: 1155516.0000 - val_mean_absolute_error: 94.4812\n",
            "Epoch 38/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1251123.3750 - mean_absolute_error: 244.9577 - val_loss: 1169678.7500 - val_mean_absolute_error: 150.7630\n",
            "Epoch 39/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1384321.2500 - mean_absolute_error: 344.6461 - val_loss: 2346875.7500 - val_mean_absolute_error: 930.1280\n",
            "Epoch 40/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2127302.0000 - mean_absolute_error: 695.2605 - val_loss: 1325982.1250 - val_mean_absolute_error: 374.0069\n",
            "Epoch 41/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1179298.3750 - mean_absolute_error: 148.8359 - val_loss: 1205937.1250 - val_mean_absolute_error: 241.7050\n",
            "Epoch 42/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 4086517.5000 - mean_absolute_error: 1125.7915 - val_loss: 1821281.7500 - val_mean_absolute_error: 714.1185\n",
            "Epoch 43/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1286945.1250 - mean_absolute_error: 260.5794 - val_loss: 1165261.2500 - val_mean_absolute_error: 153.6420\n",
            "Epoch 44/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1161449.7500 - mean_absolute_error: 137.7807 - val_loss: 1144441.1250 - val_mean_absolute_error: 114.1599\n",
            "Epoch 45/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1137928.3750 - mean_absolute_error: 87.0318 - val_loss: 1323712.5000 - val_mean_absolute_error: 387.0416\n",
            "Epoch 46/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2208898.2500 - mean_absolute_error: 682.8578 - val_loss: 3659239.2500 - val_mean_absolute_error: 1337.7682\n",
            "Epoch 47/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 3554822.0000 - mean_absolute_error: 1121.2509 - val_loss: 1242762.3750 - val_mean_absolute_error: 312.1123\n",
            "Epoch 48/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1973052.0000 - mean_absolute_error: 681.2966 - val_loss: 2311800.2500 - val_mean_absolute_error: 947.7921\n",
            "Epoch 49/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 3010451.0000 - mean_absolute_error: 941.6882 - val_loss: 9853780.0000 - val_mean_absolute_error: 2495.1345\n",
            "Epoch 50/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 4087168.5000 - mean_absolute_error: 981.2356 - val_loss: 1993825.2500 - val_mean_absolute_error: 796.7296\n",
            "Epoch 1/50\n",
            "69/69 [==============================] - 1s 12ms/step - loss: 971765184.0000 - mean_absolute_error: 8085.9360 - val_loss: 48969.8945 - val_mean_absolute_error: 75.6813\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 44331.6484 - mean_absolute_error: 52.7873 - val_loss: 37890.9102 - val_mean_absolute_error: 21.8548\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 36935.0820 - mean_absolute_error: 21.8344 - val_loss: 35761.7305 - val_mean_absolute_error: 16.6445\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 35237.6680 - mean_absolute_error: 17.1043 - val_loss: 34652.7344 - val_mean_absolute_error: 16.5847\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 34271.2930 - mean_absolute_error: 17.1484 - val_loss: 33816.9414 - val_mean_absolute_error: 16.5867\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 37081.5234 - mean_absolute_error: 40.3140 - val_loss: 34607.6680 - val_mean_absolute_error: 30.6545\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 51311.2344 - mean_absolute_error: 89.3414 - val_loss: 35758.3164 - val_mean_absolute_error: 47.8054\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 32904.4570 - mean_absolute_error: 23.6328 - val_loss: 32063.3457 - val_mean_absolute_error: 17.0006\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 31812.2129 - mean_absolute_error: 18.5321 - val_loss: 31309.1348 - val_mean_absolute_error: 15.7963\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 31011.5508 - mean_absolute_error: 15.6343 - val_loss: 30656.5801 - val_mean_absolute_error: 14.2945\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 30413.1172 - mean_absolute_error: 15.1474 - val_loss: 30193.7070 - val_mean_absolute_error: 17.3340\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 29817.3242 - mean_absolute_error: 13.5672 - val_loss: 29531.4492 - val_mean_absolute_error: 13.1668\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 29328.5703 - mean_absolute_error: 14.1812 - val_loss: 28967.9961 - val_mean_absolute_error: 11.5101\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 29889.3320 - mean_absolute_error: 24.6168 - val_loss: 29304.0996 - val_mean_absolute_error: 21.9679\n",
            "Epoch 15/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 28670.5117 - mean_absolute_error: 17.1927 - val_loss: 28285.3770 - val_mean_absolute_error: 14.3330\n",
            "Epoch 16/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 28022.8574 - mean_absolute_error: 12.6926 - val_loss: 27734.7871 - val_mean_absolute_error: 11.1589\n",
            "Epoch 17/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 27551.5566 - mean_absolute_error: 11.3986 - val_loss: 27290.5000 - val_mean_absolute_error: 10.5724\n",
            "Epoch 18/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 27247.5215 - mean_absolute_error: 13.3135 - val_loss: 26878.3379 - val_mean_absolute_error: 9.2586\n",
            "Epoch 19/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 26791.2051 - mean_absolute_error: 11.8426 - val_loss: 26749.2969 - val_mean_absolute_error: 16.6124\n",
            "Epoch 20/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 26486.8242 - mean_absolute_error: 13.0142 - val_loss: 26234.5801 - val_mean_absolute_error: 12.2746\n",
            "Epoch 21/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 26016.3789 - mean_absolute_error: 9.9498 - val_loss: 25835.3535 - val_mean_absolute_error: 10.0504\n",
            "Epoch 22/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 25721.8926 - mean_absolute_error: 10.6923 - val_loss: 25513.1973 - val_mean_absolute_error: 10.2793\n",
            "Epoch 23/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 25375.7422 - mean_absolute_error: 9.6834 - val_loss: 25180.6797 - val_mean_absolute_error: 10.0785\n",
            "Epoch 24/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 25042.0117 - mean_absolute_error: 9.2858 - val_loss: 24941.7207 - val_mean_absolute_error: 11.4087\n",
            "Epoch 25/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 25526.4883 - mean_absolute_error: 22.5413 - val_loss: 24930.1074 - val_mean_absolute_error: 16.3026\n",
            "Epoch 26/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 25300.2266 - mean_absolute_error: 23.3672 - val_loss: 24567.1289 - val_mean_absolute_error: 15.9804\n",
            "Epoch 27/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 24280.3672 - mean_absolute_error: 11.2330 - val_loss: 23987.6191 - val_mean_absolute_error: 5.8270\n",
            "Epoch 28/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 23859.8809 - mean_absolute_error: 6.3313 - val_loss: 23750.8086 - val_mean_absolute_error: 8.3698\n",
            "Epoch 29/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 23723.6211 - mean_absolute_error: 10.8599 - val_loss: 23491.4961 - val_mean_absolute_error: 8.0538\n",
            "Epoch 30/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 23405.0586 - mean_absolute_error: 8.4352 - val_loss: 23197.0156 - val_mean_absolute_error: 5.0406\n",
            "Epoch 31/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 23086.3359 - mean_absolute_error: 5.3570 - val_loss: 22938.9844 - val_mean_absolute_error: 3.6497\n",
            "Epoch 32/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 22848.5684 - mean_absolute_error: 5.2727 - val_loss: 22787.5215 - val_mean_absolute_error: 8.5813\n",
            "Epoch 33/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 22628.5332 - mean_absolute_error: 5.7924 - val_loss: 23172.8750 - val_mean_absolute_error: 22.9224\n",
            "Epoch 34/50\n",
            "69/69 [==============================] - 1s 7ms/step - loss: 22513.1582 - mean_absolute_error: 9.8146 - val_loss: 22265.5957 - val_mean_absolute_error: 5.5878\n",
            "Epoch 35/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 22151.0586 - mean_absolute_error: 4.8774 - val_loss: 22064.2031 - val_mean_absolute_error: 6.8039\n",
            "Epoch 36/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 21982.9922 - mean_absolute_error: 7.5666 - val_loss: 21804.4531 - val_mean_absolute_error: 4.0977\n",
            "Epoch 37/50\n",
            "69/69 [==============================] - 1s 15ms/step - loss: 21871.5605 - mean_absolute_error: 10.7927 - val_loss: 21956.4199 - val_mean_absolute_error: 16.4242\n",
            "Epoch 38/50\n",
            "69/69 [==============================] - 1s 12ms/step - loss: 21526.4766 - mean_absolute_error: 5.8376 - val_loss: 21361.6289 - val_mean_absolute_error: 3.2158\n",
            "Epoch 39/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 21271.7285 - mean_absolute_error: 4.3143 - val_loss: 21158.9629 - val_mean_absolute_error: 4.3604\n",
            "Epoch 40/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 21095.1289 - mean_absolute_error: 6.6028 - val_loss: 21008.2012 - val_mean_absolute_error: 7.7046\n",
            "Epoch 41/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 20865.2812 - mean_absolute_error: 5.4163 - val_loss: 20763.3906 - val_mean_absolute_error: 6.4702\n",
            "Epoch 42/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 20676.5898 - mean_absolute_error: 6.7673 - val_loss: 20663.8398 - val_mean_absolute_error: 11.4414\n",
            "Epoch 43/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 20449.9258 - mean_absolute_error: 5.7941 - val_loss: 20330.8555 - val_mean_absolute_error: 5.4956\n",
            "Epoch 44/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 20300.4570 - mean_absolute_error: 8.4532 - val_loss: 20144.7227 - val_mean_absolute_error: 6.7464\n",
            "Epoch 45/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 20032.2539 - mean_absolute_error: 5.8571 - val_loss: 19919.5742 - val_mean_absolute_error: 6.2748\n",
            "Epoch 46/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 19787.8535 - mean_absolute_error: 3.9214 - val_loss: 19714.3672 - val_mean_absolute_error: 6.6044\n",
            "Epoch 47/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 19576.0977 - mean_absolute_error: 3.7247 - val_loss: 19520.8789 - val_mean_absolute_error: 7.4806\n",
            "Epoch 48/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 19363.8652 - mean_absolute_error: 3.6559 - val_loss: 19288.9199 - val_mean_absolute_error: 6.2071\n",
            "Epoch 49/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 19433.4180 - mean_absolute_error: 13.1243 - val_loss: 19070.8125 - val_mean_absolute_error: 5.4423\n",
            "Epoch 50/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 18966.6582 - mean_absolute_error: 5.4322 - val_loss: 18882.3555 - val_mean_absolute_error: 7.1318\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for well number 1 and Oil Rate SC is fitted with RMSE: 8.1571.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for well number 1 and Gas Mass Rate(CO2) SC is fitted with RMSE: 942.0287.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for well number 1 and Water Rate SC is fitted with RMSE: 8.5850.\n",
            "Model for well number 1 is fitted with average error: 3.1987%.\n",
            "Dataset for well no. 2 is prepared for keras.\n",
            "Epoch 1/50\n",
            "69/69 [==============================] - 1s 13ms/step - loss: 106908560.0000 - mean_absolute_error: 2881.5388 - val_loss: 10155451.0000 - val_mean_absolute_error: 117.9652\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 8762539.0000 - mean_absolute_error: 179.9321 - val_loss: 7728918.5000 - val_mean_absolute_error: 190.7570\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 7158799.0000 - mean_absolute_error: 55.5667 - val_loss: 6730430.5000 - val_mean_absolute_error: 28.6391\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 6432342.0000 - mean_absolute_error: 31.4585 - val_loss: 6140883.0000 - val_mean_absolute_error: 36.3661\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 5876982.5000 - mean_absolute_error: 30.3195 - val_loss: 5608698.0000 - val_mean_absolute_error: 22.4650\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 5361511.0000 - mean_absolute_error: 20.0645 - val_loss: 5105436.0000 - val_mean_absolute_error: 24.8836\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 4871277.5000 - mean_absolute_error: 18.7323 - val_loss: 4627500.0000 - val_mean_absolute_error: 18.7068\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 4403055.5000 - mean_absolute_error: 19.2667 - val_loss: 4169379.0000 - val_mean_absolute_error: 13.4872\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 3999332.7500 - mean_absolute_error: 69.0648 - val_loss: 3938145.2500 - val_mean_absolute_error: 291.8871\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 3863640.5000 - mean_absolute_error: 227.2121 - val_loss: 3390583.2500 - val_mean_absolute_error: 36.1587\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 3183158.2500 - mean_absolute_error: 44.0019 - val_loss: 2976076.0000 - val_mean_absolute_error: 24.4007\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2797640.7500 - mean_absolute_error: 14.7828 - val_loss: 2614290.0000 - val_mean_absolute_error: 12.5552\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2453057.7500 - mean_absolute_error: 13.3146 - val_loss: 2290452.0000 - val_mean_absolute_error: 12.0233\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2143042.2500 - mean_absolute_error: 12.4353 - val_loss: 1991898.5000 - val_mean_absolute_error: 10.5163\n",
            "Epoch 15/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1868589.2500 - mean_absolute_error: 25.4892 - val_loss: 1733990.5000 - val_mean_absolute_error: 29.3430\n",
            "Epoch 16/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1612984.1250 - mean_absolute_error: 16.4281 - val_loss: 1492941.7500 - val_mean_absolute_error: 19.6741\n",
            "Epoch 17/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1387471.3750 - mean_absolute_error: 16.4795 - val_loss: 1282548.0000 - val_mean_absolute_error: 12.7161\n",
            "Epoch 18/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1188939.0000 - mean_absolute_error: 10.2632 - val_loss: 1095537.3750 - val_mean_absolute_error: 10.2291\n",
            "Epoch 19/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1014479.8750 - mean_absolute_error: 9.4318 - val_loss: 933125.2500 - val_mean_absolute_error: 8.9320\n",
            "Epoch 20/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 862089.6875 - mean_absolute_error: 9.6319 - val_loss: 791529.3750 - val_mean_absolute_error: 6.9210\n",
            "Epoch 21/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 729525.3125 - mean_absolute_error: 7.8016 - val_loss: 668108.0000 - val_mean_absolute_error: 8.3966\n",
            "Epoch 22/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 614940.3125 - mean_absolute_error: 9.4487 - val_loss: 560310.5625 - val_mean_absolute_error: 7.1052\n",
            "Epoch 23/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 515064.4375 - mean_absolute_error: 7.1240 - val_loss: 469640.1562 - val_mean_absolute_error: 7.1242\n",
            "Epoch 24/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 430508.8438 - mean_absolute_error: 9.4571 - val_loss: 390596.9688 - val_mean_absolute_error: 7.6095\n",
            "Epoch 25/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 358044.8125 - mean_absolute_error: 7.4372 - val_loss: 325417.5000 - val_mean_absolute_error: 11.9212\n",
            "Epoch 26/50\n",
            "69/69 [==============================] - 1s 14ms/step - loss: 297024.5000 - mean_absolute_error: 8.6619 - val_loss: 269377.6250 - val_mean_absolute_error: 9.2422\n",
            "Epoch 27/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 245417.4531 - mean_absolute_error: 6.2976 - val_loss: 222060.3438 - val_mean_absolute_error: 5.1100\n",
            "Epoch 28/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 202147.9844 - mean_absolute_error: 10.1329 - val_loss: 182108.0312 - val_mean_absolute_error: 7.7582\n",
            "Epoch 29/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 165907.0312 - mean_absolute_error: 11.7705 - val_loss: 150549.0625 - val_mean_absolute_error: 24.3350\n",
            "Epoch 30/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 135907.6719 - mean_absolute_error: 12.0220 - val_loss: 122001.8906 - val_mean_absolute_error: 10.0927\n",
            "Epoch 31/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 110583.7109 - mean_absolute_error: 7.2530 - val_loss: 99266.0078 - val_mean_absolute_error: 5.3609\n",
            "Epoch 32/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 89979.6797 - mean_absolute_error: 8.7165 - val_loss: 80928.6484 - val_mean_absolute_error: 15.3992\n",
            "Epoch 33/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 72890.1016 - mean_absolute_error: 9.4590 - val_loss: 65232.5859 - val_mean_absolute_error: 6.7171\n",
            "Epoch 34/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 58848.2539 - mean_absolute_error: 7.0211 - val_loss: 52562.3438 - val_mean_absolute_error: 4.2904\n",
            "Epoch 35/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 47371.3867 - mean_absolute_error: 6.9198 - val_loss: 42232.2266 - val_mean_absolute_error: 5.6202\n",
            "Epoch 36/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 38086.3477 - mean_absolute_error: 7.6665 - val_loss: 34152.4336 - val_mean_absolute_error: 12.9641\n",
            "Epoch 37/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 30680.9141 - mean_absolute_error: 11.7986 - val_loss: 27153.7871 - val_mean_absolute_error: 7.6877\n",
            "Epoch 38/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 24347.8535 - mean_absolute_error: 6.2969 - val_loss: 21569.4004 - val_mean_absolute_error: 3.6829\n",
            "Epoch 39/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 19422.6289 - mean_absolute_error: 7.5363 - val_loss: 17232.6055 - val_mean_absolute_error: 8.3300\n",
            "Epoch 40/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 15403.0156 - mean_absolute_error: 7.3943 - val_loss: 13632.3340 - val_mean_absolute_error: 6.5242\n",
            "Epoch 41/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 12342.1172 - mean_absolute_error: 10.4160 - val_loss: 10988.1143 - val_mean_absolute_error: 13.0526\n",
            "Epoch 42/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 9708.9736 - mean_absolute_error: 8.2258 - val_loss: 8445.5723 - val_mean_absolute_error: 3.9048\n",
            "Epoch 43/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 7636.6284 - mean_absolute_error: 7.8355 - val_loss: 7079.3730 - val_mean_absolute_error: 15.3317\n",
            "Epoch 44/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 6146.6255 - mean_absolute_error: 11.3227 - val_loss: 5307.3589 - val_mean_absolute_error: 7.1467\n",
            "Epoch 45/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 4667.1406 - mean_absolute_error: 5.5389 - val_loss: 4067.4702 - val_mean_absolute_error: 3.9406\n",
            "Epoch 46/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 4057.8865 - mean_absolute_error: 12.2823 - val_loss: 4103.3037 - val_mean_absolute_error: 12.1524\n",
            "Epoch 47/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 3419.5134 - mean_absolute_error: 14.7795 - val_loss: 2705.9065 - val_mean_absolute_error: 9.1730\n",
            "Epoch 48/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 2351.1362 - mean_absolute_error: 7.8571 - val_loss: 2003.9183 - val_mean_absolute_error: 5.7155\n",
            "Epoch 49/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1798.3306 - mean_absolute_error: 7.0789 - val_loss: 1576.0077 - val_mean_absolute_error: 8.2279\n",
            "Epoch 50/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 1410.8248 - mean_absolute_error: 7.6651 - val_loss: 1266.5770 - val_mean_absolute_error: 5.6973\n",
            "Epoch 1/50\n",
            "69/69 [==============================] - 1s 12ms/step - loss: 68665896960.0000 - mean_absolute_error: 127187.7891 - val_loss: 24951896.0000 - val_mean_absolute_error: 2032.7872\n",
            "Epoch 2/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 21798718.0000 - mean_absolute_error: 1421.3959 - val_loss: 19304820.0000 - val_mean_absolute_error: 862.9637\n",
            "Epoch 3/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 20249076.0000 - mean_absolute_error: 1112.3561 - val_loss: 19336604.0000 - val_mean_absolute_error: 904.5269\n",
            "Epoch 4/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 18766906.0000 - mean_absolute_error: 695.3217 - val_loss: 18249848.0000 - val_mean_absolute_error: 449.7166\n",
            "Epoch 5/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 18274196.0000 - mean_absolute_error: 468.8832 - val_loss: 18188294.0000 - val_mean_absolute_error: 470.7022\n",
            "Epoch 6/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 18203976.0000 - mean_absolute_error: 421.8136 - val_loss: 17957580.0000 - val_mean_absolute_error: 259.3896\n",
            "Epoch 7/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 18012122.0000 - mean_absolute_error: 320.6068 - val_loss: 17905978.0000 - val_mean_absolute_error: 247.8131\n",
            "Epoch 8/50\n",
            "69/69 [==============================] - 1s 9ms/step - loss: 17950322.0000 - mean_absolute_error: 305.0015 - val_loss: 17851204.0000 - val_mean_absolute_error: 211.4804\n",
            "Epoch 9/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 17829352.0000 - mean_absolute_error: 188.9826 - val_loss: 17794052.0000 - val_mean_absolute_error: 147.7636\n",
            "Epoch 10/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 17821458.0000 - mean_absolute_error: 214.5908 - val_loss: 17778054.0000 - val_mean_absolute_error: 209.1730\n",
            "Epoch 11/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 17771238.0000 - mean_absolute_error: 211.0488 - val_loss: 18010822.0000 - val_mean_absolute_error: 491.4667\n",
            "Epoch 12/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 17907370.0000 - mean_absolute_error: 384.5293 - val_loss: 17717750.0000 - val_mean_absolute_error: 232.1686\n",
            "Epoch 13/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 23060018.0000 - mean_absolute_error: 1342.0397 - val_loss: 28121548.0000 - val_mean_absolute_error: 2538.8896\n",
            "Epoch 14/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 19340218.0000 - mean_absolute_error: 776.4735 - val_loss: 17845456.0000 - val_mean_absolute_error: 378.3982\n",
            "Epoch 15/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 17784616.0000 - mean_absolute_error: 297.5159 - val_loss: 17652006.0000 - val_mean_absolute_error: 158.2688\n",
            "Epoch 16/50\n",
            "69/69 [==============================] - 1s 8ms/step - loss: 17629296.0000 - mean_absolute_error: 154.1116 - val_loss: 17602526.0000 - val_mean_absolute_error: 138.5437\n",
            "Epoch 17/50\n",
            "10/69 [===>..........................] - ETA: 0s - loss: 17859898.0000 - mean_absolute_error: 145.5746"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Make a prediction"
      ],
      "metadata": {
        "id": "0zoQGN3rR_4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Predict the testing data:\n",
        "l_ls, l_pred_ls, avg_score, rmse_ls_all, std_ls_all, time_ax = \\\n",
        "    ts_model.get_prediction(model_=model_ls_tr, var_split=var_split_ts,\n",
        "                            const_split=const_split_ts, time_split=time_split_ts,\n",
        "                            grad_split=grad_split_ts, coor_split=coor_split_ts,\n",
        "                            target_split=target_split_ts, plot_=True)\n",
        "#\n",
        "# Store the results:\n",
        "dir_fit = dir_ts + test_over_data\n",
        "for well_ in range(n_well):\n",
        "    for var_ in range(n_var):\n",
        "        f_dir = dir_fit + \"Well_\" + str(well_ + 1) + \"_\" + str(var_col[var_])\n",
        "        df_l = pd.DataFrame(l_ls[well_][var_])\n",
        "        df_l.to_csv(f_dir + \"_test.csv\")\n",
        "        df_l_pred = pd.DataFrame(l_pred_ls[well_][var_])\n",
        "        df_l_pred.to_csv(f_dir + \"_pred.csv\")\n",
        "        df_e = pd.DataFrame([avg_score[well_][var_]])\n",
        "        df_e.to_csv(f_dir + \"_error.csv\")\n",
        "        df_rmse = pd.DataFrame([rmse_ls_all[well_][var_]])\n",
        "        df_rmse.to_csv(f_dir + \"_rmse.csv\")\n",
        "        df_std = pd.DataFrame([std_ls_all[well_][var_]])\n",
        "        df_std.to_csv(f_dir + \"_std.csv\")\n",
        "        df_time = pd.DataFrame(time_ax)\n",
        "        df_time.to_csv(f_dir + \"_time.csv\")\n",
        "#"
      ],
      "metadata": {
        "id": "BW8fT-7BR--E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Make a prediction with walk-forwad method (works only for AR)."
      ],
      "metadata": {
        "id": "WICcGvJFSHUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Predict with the walk-forward:\n",
        "n_iter = np.shape(time_split_ts)[1] - (n_old_obs + 3)\n",
        "avg_score_t, l_ls, l_pred_ls, avg_score, rmse_ls_all, std_ls_all, time_ax = \\\n",
        "    ts_model.get_forecast_split(model_=model_ls_tr, n_iter=n_iter, weight_matrix=weight_matrix,\n",
        "                                var_split=var_split_ts, const_split=const_split_ts,\n",
        "                                time_split=time_split_ts, grad_split=grad_split_ts,\n",
        "                                coor_split=coor_split_ts, target_split=target_split_ts,\n",
        "                                plot_=False, new_well=False, plot_w=None)\n",
        "#\n",
        "# Store the results:\n",
        "dir_fit = dir_ts + test_walk_forward\n",
        "for well_ in range(n_well):\n",
        "    for var_ in range(n_var):\n",
        "        f_dir = dir_fit + \"Well_\" + str(well_ + 1) + \"_\" + str(var_col[var_])\n",
        "        df_l = pd.DataFrame(l_ls[well_][var_])\n",
        "        df_l.to_csv(f_dir + \"_test.csv\")\n",
        "        df_l_pred = pd.DataFrame(l_pred_ls[well_][var_])\n",
        "        df_l_pred.to_csv(f_dir + \"_pred.csv\")\n",
        "        df_e = pd.DataFrame([avg_score[well_][var_]])\n",
        "        df_e.to_csv(f_dir + \"_error.csv\")\n",
        "        df_rmse = pd.DataFrame([rmse_ls_all[well_][var_]])\n",
        "        df_rmse.to_csv(f_dir + \"_rmse.csv\")\n",
        "        df_std = pd.DataFrame([std_ls_all[well_][var_]])\n",
        "        df_std.to_csv(f_dir + \"_std.csv\")\n",
        "        df_time = pd.DataFrame(time_ax)\n",
        "        df_time.to_csv(f_dir + \"_time.csv\")\n",
        "#\n",
        "'''"
      ],
      "metadata": {
        "id": "KUGul0RnSHpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Generate new well."
      ],
      "metadata": {
        "id": "B1hG8XXjSTGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the information for the new well:\n",
        "dir_n_well = ts_model.dir_n_well\n",
        "pred_df_r_ls, pred_df_ls, norm_ = ts_model.get_df_list(dir_=dir_n_well, norm_=norm_,\n",
        "                                                       init_=False, ext_1=\"PRED-\", roll=False, pro_=\"03\")\n",
        "# Generate the new well (5 years => 1827 days) from the dataset:\n",
        "l_pred_, time_ax = ts_model.pipeline_new_well_over_data(df_init_ls=pro_df_ls, df_new_ls=pred_df_ls,\n",
        "                                                        model_ls_tr=model_ls_tr, n_iter=1825,\n",
        "                                                        new_factor=2, plot_=True)\n",
        "#\n",
        "# Store the results:\n",
        "dir_fit = dir_ts + new_well_over_data\n",
        "for var_ in range(n_var):\n",
        "    f_dir = dir_fit + \"PRED_\" + \"_\" + str(var_col[var_])\n",
        "    df_l_pred = pd.DataFrame(l_pred_[var_])\n",
        "    df_l_pred.to_csv(f_dir + \"_pred.csv\")\n",
        "    df_time = pd.DataFrame(time_ax)\n",
        "    df_time.to_csv(f_dir + \"_time.csv\")\n",
        "#"
      ],
      "metadata": {
        "id": "T0b1P95nSSbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Generate new well with walk forward method (works only for AR)."
      ],
      "metadata": {
        "id": "mSBfgH0FSY_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Generate the new well (5 years => 1827 days):\n",
        "l_pred_, l_pred_ls, time_ax = ts_model.pipeline_new_well(df_init_ls=pro_df_ls, df_new_ls=pred_df_ls,\n",
        "                                                         model_ls_tr=model_ls_tr, n_iter=1825,\n",
        "                                                         new_factor=3, plot_=False)\n",
        "#\n",
        "# Store the results:\n",
        "dir_fit = dir_ts + new_well_forward\n",
        "for var_ in range(n_var):\n",
        "    f_dir = dir_fit + \"PRED_\" + \"_\" + str(var_col[var_])\n",
        "    df_l_pred = pd.DataFrame(l_pred_[var_])\n",
        "    df_l_pred.to_csv(f_dir + \"_pred.csv\")\n",
        "    df_time = pd.DataFrame(time_ax)\n",
        "    df_time.to_csv(f_dir + \"_time.csv\")\n",
        "\n",
        "#'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "YOExb_X3dLm4",
        "outputId": "2fac4406-0a95-4003-d605-443a37872e02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6250ce02b8c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the model validation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m a_ls_tr, l_ls, l_pred_ls, model_ls_tr, avg_score, rmse_ls_all, std_ls_all, time_ax =     ts_model.pipeline_model_split(var_split=var_split_tr, const_split=const_split_tr,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0mtime_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_split_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_split_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   coor_split=coor_split_tr, target_split=target_split_tr, plot_=True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ts_model' is not defined"
          ]
        }
      ]
    }
  ]
}